"""
NDCGè¯„ä¼°è„šæœ¬ï¼šä½¿ç”¨PairingNetçš„NDCGæ–¹æ³•è¯„ä¼°EdgeSpark final_approachæ¨¡å‹
åŸºäºPairingNet/PairingNet Code/utils/NDCG.pyçš„è¯„ä¼°æ–¹æ³•
"""
import torch
import numpy as np
import pickle
import os
import sys
import argparse
from tqdm import tqdm
import json
from datetime import datetime

# æ·»åŠ è·¯å¾„ä»¥å¯¼å…¥EdgeSparkæ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src', 'models'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'src', 'data'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'PairingNet', 'PairingNet Code', 'utils'))

from final_approach import EdgeMatchingNet
from improved_dataset_loader import create_improved_dataloaders
from NDCG import calculate_NDCG, ndcg

class EdgeSparkNDCGEvaluator:
    """
    EdgeSpark NDCGè¯„ä¼°å™¨
    ä½¿ç”¨PairingNetçš„NDCGè¯„ä¼°æ–¹æ³•æ¥è¯„ä¼°final_approachæ¨¡å‹
    """
    
    def __init__(self, model_path, device=None):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ (cuda/cpu)
        """
        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_path = model_path
        self.model = None
        
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        self.model = EdgeMatchingNet(max_points=1000).to(self.device)
        
        # åŠ è½½æƒé‡
        if os.path.exists(self.model_path):
            state_dict = torch.load(self.model_path, map_location=self.device)
            self.model.load_state_dict(state_dict)
            self.model.eval()
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        else:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {self.model_path}")
    
    def load_dataset(self, dataset_path):
        """
        åŠ è½½æ•°æ®é›†
        
        Args:
            dataset_path: æ•°æ®é›†pklæ–‡ä»¶è·¯å¾„
            
        Returns:
            tuple: (fragments, gt_pairs)
        """
        print(f"ğŸ“š åŠ è½½æ•°æ®é›†: {dataset_path}")
        
        with open(dataset_path, 'rb') as f:
            data = pickle.load(f)
        
        fragments = data['full_pcd_all']  # è¾¹ç¼˜ç‚¹äº‘æ•°æ®
        gt_pairs = data['GT_pairs']       # çœŸå®åŒ¹é…å¯¹
        
        print(f"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
        print(f"   - ç‰‡æ®µæ•°é‡: {len(fragments)}")
        print(f"   - çœŸå®åŒ¹é…å¯¹: {len(gt_pairs)}")
        
        return fragments, gt_pairs
    
    def preprocess_fragment(self, fragment, max_points=1000):
        """
        é¢„å¤„ç†ç‰‡æ®µæ•°æ®
        
        Args:
            fragment: ç‰‡æ®µè¾¹ç¼˜ç‚¹äº‘ (numpy array)
            max_points: æœ€å¤§ç‚¹æ•°
            
        Returns:
            torch.Tensor: é¢„å¤„ç†åçš„ç‚¹äº‘ (1, max_points, 2)
        """
        # ç¡®ä¿æ˜¯numpyæ•°ç»„
        if not isinstance(fragment, np.ndarray):
            fragment = np.array(fragment)
        
        # ç¡®ä¿æ˜¯2Dç‚¹äº‘ (N, 2)
        if fragment.shape[1] != 2:
            fragment = fragment[:, :2]  # åªå–å‰ä¸¤ç»´
        
        # é™åˆ¶ç‚¹æ•°
        if len(fragment) > max_points:
            # ç­‰é—´éš”é‡‡æ ·
            indices = np.linspace(0, len(fragment)-1, max_points, dtype=int)
            fragment = fragment[indices]
        elif len(fragment) < max_points:
            # è¡¥é½åˆ°max_points
            padding_size = max_points - len(fragment)
            if len(fragment) > 0:
                # é‡å¤æœ€åä¸€ä¸ªç‚¹
                last_point = fragment[-1:].repeat(padding_size, axis=0)
                fragment = np.vstack([fragment, last_point])
            else:
                # å¦‚æœç‰‡æ®µä¸ºç©ºï¼Œç”¨é›¶ç‚¹å¡«å……
                fragment = np.zeros((max_points, 2))
        
        # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
        fragment_tensor = torch.FloatTensor(fragment).unsqueeze(0)  # (1, max_points, 2)
        
        return fragment_tensor
    
    def compute_similarity_matrix(self, fragments, batch_size=64, sample_size=None):
        """
        è®¡ç®—æ‰€æœ‰ç‰‡æ®µå¯¹çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        
        Args:
            fragments: ç‰‡æ®µåˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            sample_size: é‡‡æ ·ç‰‡æ®µæ•°é‡ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨ï¼‰
            
        Returns:
            numpy.ndarray: ç›¸ä¼¼åº¦çŸ©é˜µ (n_fragments, n_fragments)
        """
        print("ğŸ”„ è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
        
        # å¯é€‰é‡‡æ ·ä»¥åŠ é€Ÿæµ‹è¯•
        fragment_indices = None
        if sample_size and sample_size < len(fragments):
            print(f"âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šé‡‡æ · {sample_size} ä¸ªç‰‡æ®µ")
            fragment_indices = np.random.choice(len(fragments), sample_size, replace=False)
            fragments = [fragments[i] for i in fragment_indices]
        
        n_fragments = len(fragments)
        similarity_matrix = np.zeros((n_fragments, n_fragments))
        
        print(f"ğŸ“Š è®¡ç®—è§„æ¨¡: {n_fragments} x {n_fragments} = {n_fragments*n_fragments:,} ä¸ªç‰‡æ®µå¯¹")
        print(f"â±ï¸  é¢„ä¼°æ—¶é—´: ~{(n_fragments * 2.0 / 60):.1f} åˆ†é’Ÿ")
        
        # é¢„å¤„ç†æ‰€æœ‰ç‰‡æ®µ
        print("   é¢„å¤„ç†ç‰‡æ®µ...")
        processed_fragments = []
        for i, fragment in enumerate(tqdm(fragments, desc="é¢„å¤„ç†")):
            processed = self.preprocess_fragment(fragment).to(self.device)
            processed_fragments.append(processed)
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨æ‰¹å¤„ç†ä»¥æé«˜æ•ˆç‡ï¼‰
        print("   è®¡ç®—ç‰‡æ®µç›¸ä¼¼åº¦...")
        with torch.no_grad():
            for i in tqdm(range(n_fragments), desc="è®¡ç®—ç›¸ä¼¼åº¦"):
                fragment_i = processed_fragments[i]
                
                # æ‰¹å¤„ç†è®¡ç®—ä¸å…¶ä»–æ‰€æœ‰ç‰‡æ®µçš„ç›¸ä¼¼åº¦
                for start_j in range(0, n_fragments, batch_size):
                    end_j = min(start_j + batch_size, n_fragments)
                    
                    # åˆ›å»ºæ‰¹æ¬¡
                    batch_fragments_i = fragment_i.repeat(end_j - start_j, 1, 1)
                    batch_fragments_j = torch.cat([processed_fragments[j] for j in range(start_j, end_j)], dim=0)
                    
                    # è®¡ç®—åŒ¹é…æ¦‚ç‡
                    logits = self.model(batch_fragments_i, batch_fragments_j)
                    probs = torch.sigmoid(logits).squeeze().cpu().numpy()
                    
                    # å¦‚æœåªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œç¡®ä¿æ˜¯æ•°ç»„
                    if np.isscalar(probs):
                        probs = np.array([probs])
                    
                    # å­˜å‚¨ç›¸ä¼¼åº¦
                    similarity_matrix[i, start_j:end_j] = probs
        
        print("âœ… ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å®Œæˆ")
        return similarity_matrix, fragment_indices
    
    def evaluate_ndcg(self, similarity_matrix, gt_pairs, fragment_indices=None, save_results=True, output_dir="./results"):
        """
        ä½¿ç”¨PairingNetçš„æ–¹æ³•è¯„ä¼°NDCG
        
        Args:
            similarity_matrix: ç›¸ä¼¼åº¦çŸ©é˜µ
            gt_pairs: çœŸå®åŒ¹é…å¯¹
            fragment_indices: é‡‡æ ·çš„ç‰‡æ®µç´¢å¼•ï¼ˆç”¨äºé‡‡æ ·æ¨¡å¼ï¼‰
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            output_dir: ç»“æœä¿å­˜ç›®å½•
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        print("ğŸ“Š å¼€å§‹NDCGè¯„ä¼°...")
        
        # å¦‚æœæ˜¯é‡‡æ ·æ¨¡å¼ï¼Œéœ€è¦è¿‡æ»¤GT pairs
        if fragment_indices is not None:
            print(f"ğŸ” è¿‡æ»¤é‡‡æ ·ç‰‡æ®µçš„GT pairs...")
            # åªä¿ç•™åœ¨é‡‡æ ·ç´¢å¼•ä¸­çš„GT pairs
            filtered_gt_pairs = []
            index_set = set(fragment_indices)
            for source_idx, target_idx in gt_pairs:
                if source_idx in index_set and target_idx in index_set:
                    # é‡æ–°æ˜ å°„ç´¢å¼•
                    new_source = list(fragment_indices).index(source_idx)
                    new_target = list(fragment_indices).index(target_idx)
                    filtered_gt_pairs.append([new_source, new_target])
            gt_pairs = filtered_gt_pairs
            print(f"   è¿‡æ»¤åGT pairs: {len(gt_pairs)}")
        
        # ä½¿ç”¨PairingNetçš„calculate_NDCGå‡½æ•°
        print("\n=== PairingNet NDCG è¯„ä¼°ç»“æœ ===")
        if len(gt_pairs) > 0:
            calculate_NDCG(similarity_matrix, gt_pairs)
        else:
            print("âš ï¸  é‡‡æ ·ä¸­æ²¡æœ‰æœ‰æ•ˆçš„GT pairsï¼Œè·³è¿‡PairingNetè¯„ä¼°")
        
        # æ‰‹åŠ¨è®¡ç®—è¯¦ç»†çš„NDCGç»“æœä»¥è·å–æ•°å€¼
        print("\nğŸ“ˆ è¯¦ç»†NDCGè®¡ç®—...")
        
        # æŒ‰ç…§PairingNetçš„æ–¹æ³•åˆ›å»ºæ’åºç´¢å¼•
        idx = np.argsort(-similarity_matrix, axis=1)
        length = similarity_matrix.shape[0]
        
        # åˆ›å»ºground truthå’ŒpredictionçŸ©é˜µ
        gt = np.zeros((length, length), dtype=np.uint8)
        pred = np.zeros((length, length), dtype=np.uint8)
        
        for i in range(len(gt_pairs)):
            source_idx = gt_pairs[i][0]
            target_idx = gt_pairs[i][1]
            
            # æ‰¾åˆ°targetåœ¨sourceçš„æ’åºä¸­çš„ä½ç½®
            location = np.argwhere(idx[source_idx] == target_idx)[0][0]
            
            # è®¾ç½®ground truthå’Œprediction
            gt[source_idx, target_idx] = 1
            pred[source_idx, location] = 1
        
        # ç§»é™¤å…¨é›¶è¡Œï¼ˆæ²¡æœ‰åŒ¹é…å¯¹çš„ç‰‡æ®µï¼‰
        def remove_zero_rows(array):
            mask = np.all(array == 0, axis=1)
            return array[~mask]
        
        new_gt = remove_zero_rows(gt)
        new_pred = remove_zero_rows(pred)
        
        # è®¡ç®—NDCG@5, @10, @20
        ndcg_5 = ndcg(new_gt, new_pred, 5)
        ndcg_10 = ndcg(new_gt, new_pred, 10)
        ndcg_20 = ndcg(new_gt, new_pred, 20)
        
        results = {
            'NDCG@5': float(ndcg_5),
            'NDCG@10': float(ndcg_10),
            'NDCG@20': float(ndcg_20),
            'total_fragments': int(length),
            'total_gt_pairs': int(len(gt_pairs)),
            'fragments_with_matches': int(np.sum(~np.all(gt == 0, axis=1)))
        }
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š EdgeSpark NDCG è¯„ä¼°ç»“æœ:")
        print(f"   NDCG@5:  {ndcg_5:.4f}")
        print(f"   NDCG@10: {ndcg_10:.4f}")
        print(f"   NDCG@20: {ndcg_20:.4f}")
        print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"   æ€»ç‰‡æ®µæ•°: {length}")
        print(f"   çœŸå®åŒ¹é…å¯¹: {len(gt_pairs)}")
        print(f"   æœ‰åŒ¹é…çš„ç‰‡æ®µ: {results['fragments_with_matches']}")
        
        # ä¿å­˜ç»“æœ
        if save_results:
            os.makedirs(output_dir, exist_ok=True)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜JSONç»“æœ
            result_file = os.path.join(output_dir, f"ndcg_evaluation_{timestamp}.json")
            with open(result_file, 'w') as f:
                json.dump(results, f, indent=2)
            
            # ä¿å­˜ç›¸ä¼¼åº¦çŸ©é˜µ
            matrix_file = os.path.join(output_dir, f"similarity_matrix_{timestamp}.npy")
            np.save(matrix_file, similarity_matrix)
            
            print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜:")
            print(f"   - JSONç»“æœ: {result_file}")
            print(f"   - ç›¸ä¼¼åº¦çŸ©é˜µ: {matrix_file}")
        
        return results
    
    def run_evaluation(self, dataset_path, save_results=True, output_dir="./results"):
        """
        è¿è¡Œå®Œæ•´çš„NDCGè¯„ä¼°æµç¨‹
        
        Args:
            dataset_path: æ•°æ®é›†è·¯å¾„
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
            output_dir: ç»“æœä¿å­˜ç›®å½•
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        print("ğŸš€ å¼€å§‹EdgeSpark NDCGè¯„ä¼°")
        print("=" * 60)
        
        # 1. åŠ è½½æ¨¡å‹
        self.load_model()
        
        # 2. åŠ è½½æ•°æ®é›†
        fragments, gt_pairs = self.load_dataset(dataset_path)
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix, fragment_indices = self.compute_similarity_matrix(fragments, sample_size=100)  # è¶…å¿«é€Ÿæµ‹è¯•
        
        # 4. è¯„ä¼°NDCG
        results = self.evaluate_ndcg(similarity_matrix, gt_pairs, fragment_indices, save_results, output_dir)
        
        print("\nğŸ‰ è¯„ä¼°å®Œæˆ!")
        return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="EdgeSpark NDCGè¯„ä¼°")
    parser.add_argument("--model", type=str, required=True, help="æ¨¡å‹æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dataset", type=str, required=True, help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„") 
    parser.add_argument("--output", type=str, default="./results", help="ç»“æœä¿å­˜ç›®å½•")
    parser.add_argument("--device", type=str, default="auto", help="è®¡ç®—è®¾å¤‡ (cuda/cpu/auto)")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device == "auto":
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EdgeSparkNDCGEvaluator(args.model, device)
    
    # è¿è¡Œè¯„ä¼°
    try:
        results = evaluator.run_evaluation(args.dataset, save_results=True, output_dir=args.output)
        print(f"\nâœ… è¯„ä¼°æˆåŠŸå®Œæˆ!")
        print(f"æœ€ç»ˆNDCGç»“æœ: @5={results['NDCG@5']:.4f}, @10={results['NDCG@10']:.4f}, @20={results['NDCG@20']:.4f}")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
        raise

if __name__ == "__main__":
    main()