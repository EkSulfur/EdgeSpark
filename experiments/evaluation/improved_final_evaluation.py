#!/usr/bin/env python3
"""
æ”¹è¿›çš„æœ€ç»ˆè¯„ä¼°è„šæœ¬
è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæä¾›æ›´å¯é çš„æ€§èƒ½è¯„ä¼°
"""
import torch
import torch.nn as nn
import torch.optim as optim
import time
import sys
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('/home/eksulfur/EdgeSpark')

from final_improvements.high_sampling_approach import HighSamplingEdgeMatchingNet
from final_improvements.fourier_approach import FourierBasedMatchingNet, HybridFourierNet
from simplified_approach.dataset_simple import create_simple_dataloaders

def robust_evaluation():
    """æ›´å¯é çš„è¯„ä¼°æ–¹æ¡ˆ"""
    print("ğŸ¯ EdgeSpark æœ€ç»ˆæ”¹è¿›æ–¹æ¡ˆç¨³å¥è¯„ä¼°")
    print("=" * 70)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆæ›´å¤§çš„éªŒè¯é›†ï¼‰
    print("\nğŸ“š å‡†å¤‡æ•°æ®...")
    try:
        train_loader, val_loader, test_loader = create_simple_dataloaders(
            "dataset/train_set.pkl",
            "dataset/valid_set.pkl",
            "dataset/test_set.pkl",
            batch_size=16,
            max_points=1000,
            num_workers=2
        )
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)}")
        print(f"   éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
    except Exception as e:
        print(f"   æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return []
    
    # æµ‹è¯•é…ç½®ï¼ˆæ›´ä¿å®ˆçš„å‚æ•°ä»¥é¿å…è¿‡æ‹Ÿåˆï¼‰
    models = [
        {
            'name': 'é«˜é‡‡æ ·æ–¹æ¡ˆ(10é‡‡æ ·)',
            'model': HighSamplingEdgeMatchingNet(segment_length=50, num_samples=10, feature_dim=128),
            'lr': 0.0005,
            'weight_decay': 1e-3
        },
        {
            'name': 'é«˜é‡‡æ ·æ–¹æ¡ˆ(15é‡‡æ ·)',
            'model': HighSamplingEdgeMatchingNet(segment_length=50, num_samples=15, feature_dim=128),
            'lr': 0.0005,
            'weight_decay': 1e-3
        },
        {
            'name': 'åŸºç¡€å‚…é‡Œå¶æ–¹æ¡ˆ',
            'model': FourierBasedMatchingNet(max_points=1000, num_freqs=64, feature_dim=128),
            'lr': 0.001,
            'weight_decay': 1e-3
        },
        {
            'name': 'æ··åˆå‚…é‡Œå¶æ–¹æ¡ˆ',
            'model': HybridFourierNet(max_points=1000, num_freqs=64, feature_dim=128, num_samples=5),
            'lr': 0.0008,
            'weight_decay': 1e-3
        }
    ]
    
    results = []
    
    for model_config in models:
        print(f"\nğŸ§ª æµ‹è¯•: {model_config['name']}")
        print("-" * 50)
        
        model = model_config['model'].to(device)
        optimizer = optim.Adam(
            model.parameters(), 
            lr=model_config['lr'], 
            weight_decay=model_config['weight_decay']
        )
        criterion = nn.BCEWithLogitsLoss()
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.7, patience=3, verbose=True
        )
        
        # æ›´é•¿çš„è®­ç»ƒï¼Œæ—©åœæœºåˆ¶
        best_acc = 0.0
        best_f1 = 0.0
        best_auc = 0.0
        training_time = 0.0
        patience_counter = 0
        patience_limit = 5
        
        for epoch in range(20):  # æ›´å¤šè½®æ•°
            epoch_start = time.time()
            
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_preds = []
            train_labels = []
            train_loss = 0.0
            
            # é™åˆ¶æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°ä»¥ä¿æŒåˆç†çš„è®­ç»ƒæ—¶é—´
            max_batches = min(100, len(train_loader))
            
            for batch_idx, batch in enumerate(train_loader):
                if batch_idx >= max_batches:
                    break
                    
                points1 = batch['source_points'].to(device)
                points2 = batch['target_points'].to(device)
                labels = batch['label'].to(device)
                
                optimizer.zero_grad()
                logits = model(points1, points2)
                loss = criterion(logits, labels)
                
                # æ·»åŠ L2æ­£åˆ™åŒ–
                l2_reg = torch.tensor(0.).to(device)
                for param in model.parameters():
                    l2_reg += torch.norm(param, p=2)
                loss = loss + 1e-5 * l2_reg
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                train_loss += loss.item()
                
                # è®¡ç®—é¢„æµ‹ç»“æœ
                with torch.no_grad():
                    probs = torch.sigmoid(logits)
                    preds = (probs > 0.5).float()
                    train_preds.extend(preds.cpu().numpy())
                    train_labels.extend(labels.cpu().numpy())
            
            train_acc = accuracy_score(train_labels, train_preds) if train_preds else 0.0
            train_loss /= max_batches
            
            # éªŒè¯é˜¶æ®µï¼ˆä½¿ç”¨å®Œæ•´éªŒè¯é›†ï¼‰
            model.eval()
            val_preds = []
            val_probs = []
            val_labels = []
            val_loss = 0.0
            
            with torch.no_grad():
                for batch in val_loader:
                    points1 = batch['source_points'].to(device)
                    points2 = batch['target_points'].to(device)
                    labels = batch['label'].to(device)
                    
                    logits = model(points1, points2)
                    loss = criterion(logits, labels)
                    val_loss += loss.item()
                    
                    probs = torch.sigmoid(logits)
                    preds = (probs > 0.5).float()
                    val_preds.extend(preds.cpu().numpy())
                    val_probs.extend(probs.cpu().numpy())
                    val_labels.extend(labels.cpu().numpy())
            
            val_loss /= len(val_loader)
            val_acc = accuracy_score(val_labels, val_preds) if val_preds else 0.0
            
            # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
            try:
                precision, recall, f1, _ = precision_recall_fscore_support(
                    val_labels, val_preds, average='binary', zero_division=0
                )
                auc = roc_auc_score(val_labels, val_probs)
            except:
                precision, recall, f1, auc = 0.0, 0.0, 0.0, 0.5
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            improved = False
            if val_acc > best_acc:
                best_acc = val_acc
                best_f1 = f1
                best_auc = auc
                improved = True
                patience_counter = 0
            else:
                patience_counter += 1
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_acc)
            
            epoch_time = time.time() - epoch_start
            training_time += epoch_time
            
            print(f"   Epoch {epoch+1:2d}: Train={train_acc:.4f}({train_loss:.4f}), "
                  f"Val={val_acc:.4f}({val_loss:.4f}), F1={f1:.4f}, AUC={auc:.4f}, "
                  f"Time={epoch_time:.1f}s {'ğŸŒŸ' if improved else ''}")
            
            # æ—©åœ
            if patience_counter >= patience_limit:
                print(f"   æ—©åœæœºåˆ¶è§¦å‘ (patience={patience_limit})")
                break
        
        result = {
            'name': model_config['name'],
            'best_acc': best_acc,
            'best_f1': best_f1,
            'best_auc': best_auc,
            'training_time': training_time,
            'params': sum(p.numel() for p in model.parameters()),
            'epochs_trained': epoch + 1,
            'avg_epoch_time': training_time / (epoch + 1)
        }
        
        results.append(result)
        print(f"   âœ… æœ€ä½³ç»“æœ: å‡†ç¡®ç‡={best_acc:.4f}, F1={best_f1:.4f}, AUC={best_auc:.4f}")
    
    # åˆ†æå’Œæ€»ç»“ç»“æœ
    print(f"\nğŸ“Š ç¨³å¥è¯„ä¼°ç»“æœæ€»ç»“")
    print("=" * 90)
    
    results.sort(key=lambda x: x['best_acc'], reverse=True)
    
    print(f"{'æ’å':<4} {'æ–¹æ³•':<25} {'å‡†ç¡®ç‡':<8} {'F1':<8} {'AUC':<8} {'å‚æ•°é‡':<10} {'è®­ç»ƒè½®æ•°':<8}")
    print("-" * 90)
    
    for i, result in enumerate(results):
        print(f"{i+1:<4} {result['name']:<25} {result['best_acc']:.4f}   "
              f"{result['best_f1']:.4f}   {result['best_auc']:.4f}   "
              f"{result['params']:,}   {result['epochs_trained']:<8}")
    
    # ä¸å†å²åŸºå‡†è¯¦ç»†å¯¹æ¯”
    print(f"\nğŸ“ˆ ä¸å†å²åŸºå‡†è¯¦ç»†å¯¹æ¯”")
    print("-" * 60)
    
    baselines = [
        ('åŸå§‹å¤æ‚ç½‘ç»œ', 0.5000, 0.5000, 0.5000),
        ('ç®€åŒ–ç½‘ç»œ', 0.5985, 0.5800, 0.6200), 
        ('final_approach', 0.6095, 0.6000, 0.6500),
        ('æ··åˆæ–¹æ³•(å•é‡‡æ ·)', 0.5839, 0.5750, 0.6100)
    ]
    
    print(f"{'æ–¹æ³•':<25} {'å‡†ç¡®ç‡':<8} {'F1':<8} {'AUC':<8} {'çŠ¶æ€':<10}")
    print("-" * 60)
    for method, acc, f1, auc in baselines:
        print(f"{method:<25} {acc:.4f}   {f1:.4f}   {auc:.4f}   å†å²åŸºå‡†")
    
    if results:
        best_result = results[0]
        print(f"{best_result['name']:<25} {best_result['best_acc']:.4f}   "
              f"{best_result['best_f1']:.4f}   {best_result['best_auc']:.4f}   ğŸ† æœ€ä½³æ–°æ–¹æ³•")
        
        # è¯¦ç»†æ”¹è¿›åˆ†æ
        print(f"\nğŸ’¡ è¯¦ç»†æ”¹è¿›åˆ†æ")
        print("-" * 50)
        
        baseline_acc = 0.6095  # final_approach
        improvement = best_result['best_acc'] - baseline_acc
        
        print(f"ğŸ¯ æœ€ä½³æ–¹æ³•: {best_result['name']}")
        print(f"ğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å‡†ç¡®ç‡: {best_result['best_acc']:.4f} (vs {baseline_acc:.4f})")
        print(f"   F1-Score: {best_result['best_f1']:.4f}")
        print(f"   AUC: {best_result['best_auc']:.4f}")
        print(f"   æ”¹è¿›å¹…åº¦: {improvement:+.4f} ({improvement/baseline_acc*100:+.1f}%)")
        
        print(f"\nâš¡ æ•ˆç‡åˆ†æ:")
        print(f"   å‚æ•°æ•°é‡: {best_result['params']:,}")
        print(f"   è®­ç»ƒè½®æ•°: {best_result['epochs_trained']}")
        print(f"   å¹³å‡æ¯è½®æ—¶é—´: {best_result['avg_epoch_time']:.1f}s")
        
        # è¯„ä¼°ç”¨æˆ·å»ºè®®çš„æœ‰æ•ˆæ€§
        print(f"\nğŸ” ç”¨æˆ·å»ºè®®æœ‰æ•ˆæ€§åˆ†æ:")
        
        high_sampling_results = [r for r in results if 'é«˜é‡‡æ ·' in r['name']]
        fourier_results = [r for r in results if 'å‚…é‡Œå¶' in r['name']]
        
        if high_sampling_results:
            best_high_sampling = max(high_sampling_results, key=lambda x: x['best_acc'])
            hs_improvement = best_high_sampling['best_acc'] - baseline_acc
            print(f"   é«˜é‡‡æ ·ç­–ç•¥: {hs_improvement:+.4f} æ”¹è¿›")
            
        if fourier_results:
            best_fourier = max(fourier_results, key=lambda x: x['best_acc'])
            fourier_improvement = best_fourier['best_acc'] - baseline_acc
            print(f"   å‚…é‡Œå¶å˜æ¢: {fourier_improvement:+.4f} æ”¹è¿›")
        
        # æ€»ä½“è¯„ä¼°
        if improvement > 0.02:
            print(f"\nâœ… æ€»ä½“è¯„ä¼°: æ˜¾è‘—æ”¹è¿› - ç”¨æˆ·å»ºè®®éå¸¸æœ‰æ•ˆ!")
            success_level = "æ˜¾è‘—æˆåŠŸ"
        elif improvement > 0.01:
            print(f"\nâœ… æ€»ä½“è¯„ä¼°: æ˜æ˜¾æ”¹è¿› - ç”¨æˆ·å»ºè®®æœ‰æ•ˆ")
            success_level = "æˆåŠŸ"
        elif improvement > 0.005:
            print(f"\nâš ï¸ æ€»ä½“è¯„ä¼°: å°å¹…æ”¹è¿› - æ–¹å‘æ­£ç¡®ï¼Œéœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
            success_level = "éƒ¨åˆ†æˆåŠŸ"
        else:
            print(f"\nâŒ æ€»ä½“è¯„ä¼°: æ— æ˜æ˜¾æ”¹è¿› - éœ€è¦é‡æ–°è€ƒè™‘ç­–ç•¥")
            success_level = "éœ€è¦æ”¹è¿›"
            
        results[0]['success_level'] = success_level
    
    return results

if __name__ == "__main__":
    results = robust_evaluation()