import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import os
import time
import json
import argparse
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
import numpy as np

from network_simple import SimpleEdgeSparkNet
from dataset_simple import create_simple_dataloaders

class FocalLoss(nn.Module):
    """
    Focal Lossç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    """
    def __init__(self, alpha=0.25, gamma=2.0):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        
    def forward(self, inputs, targets):
        # è®¡ç®—BCE loss
        bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        
        # è®¡ç®—æ¦‚ç‡
        pt = torch.exp(-bce_loss)
        
        # è®¡ç®—alphaæƒé‡
        alpha_weight = targets * self.alpha + (1 - targets) * (1 - self.alpha)
        
        # è®¡ç®—focalæƒé‡
        focal_weight = (1 - pt) ** self.gamma
        
        # æœ€ç»ˆloss
        focal_loss = alpha_weight * focal_weight * bce_loss
        
        return focal_loss.mean()

class SimpleTrainer:
    """ç®€åŒ–ç‰ˆè®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = config['save_dir']
        os.makedirs(self.save_dir, exist_ok=True)
        
        # åˆ›å»ºTensorBoard
        self.writer = SummaryWriter(os.path.join(self.save_dir, 'tensorboard'))
        
        # åˆå§‹åŒ–ç½‘ç»œ
        self.model = SimpleEdgeSparkNet(**config['model']).to(self.device)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(  # ä½¿ç”¨AdamW
            self.model.parameters(),
            lr=config['training']['learning_rate'],
            weight_decay=config['training']['weight_decay'],
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - æ”¹ä¸ºç®€å•çš„StepLR
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=10,
            gamma=0.5
        )
        
        # æŸå¤±å‡½æ•°
        if config['training']['use_focal_loss']:
            self.criterion = FocalLoss(alpha=0.25, gamma=2.0)
        else:
            self.criterion = nn.BCEWithLogitsLoss()
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'epoch': [],
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'val_f1': [],
            'val_auc': [],
            'lr': []
        }
        
        # æœ€ä½³æ¨¡å‹è·Ÿè¸ª
        self.best_val_acc = 0.0
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        
        for batch_idx, batch in enumerate(train_loader):
            # æ•°æ®å‡†å¤‡
            source_points = batch['source_points'].to(self.device)
            target_points = batch['target_points'].to(self.device)
            labels = batch['label'].to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            predictions = self.model(source_points, target_points)
            
            # è®¡ç®—æŸå¤±
            loss = self.criterion(predictions, labels)
            
            # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"  Warning: è·³è¿‡æ— æ•ˆæŸå¤± {loss.item()}")
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # å‚æ•°æ›´æ–°
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            probs = torch.sigmoid(predictions)
            preds = (probs > 0.5).float()
            
            all_predictions.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            # è¿›åº¦æ˜¾ç¤º
            if batch_idx % 50 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f'  Batch {batch_idx:3d}/{len(train_loader):3d} | '
                      f'Loss: {loss.item():.4f} | LR: {current_lr:.6f}')
        
        # è®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(train_loader)
        accuracy = accuracy_score(all_labels, all_predictions)
        
        return avg_loss, accuracy
    
    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        
        total_loss = 0.0
        all_predictions = []
        all_probabilities = []
        all_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                # æ•°æ®å‡†å¤‡
                source_points = batch['source_points'].to(self.device)
                target_points = batch['target_points'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                predictions = self.model(source_points, target_points)
                
                # è®¡ç®—æŸå¤±
                loss = self.criterion(predictions, labels)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                probs = torch.sigmoid(predictions)
                preds = (probs > 0.5).float()
                
                all_predictions.extend(preds.cpu().numpy())
                all_probabilities.extend(probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # è®¡ç®—æŒ‡æ ‡
        avg_loss = total_loss / len(val_loader)
        accuracy = accuracy_score(all_labels, all_predictions)
        
        try:
            precision, recall, f1, _ = precision_recall_fscore_support(
                all_labels, all_predictions, average='binary', zero_division=0
            )
            auc = roc_auc_score(all_labels, all_probabilities)
        except:
            precision, recall, f1, auc = 0.0, 0.0, 0.0, 0.5
        
        return avg_loss, accuracy, precision, recall, f1, auc
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_acc': self.best_val_acc,
            'best_val_loss': self.best_val_loss,
            'train_history': self.train_history,
            'config': self.config
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = os.path.join(self.save_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint, latest_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = os.path.join(self.save_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: acc={self.best_val_acc:.4f}")
    
    def train(self, train_loader, val_loader):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒç®€åŒ–ç‰ˆEdgeSpark")
        print(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        
        start_time = time.time()
        
        for epoch in range(self.config['training']['epochs']):
            epoch_start = time.time()
            print(f"\nğŸ“Š Epoch {epoch+1}/{self.config['training']['epochs']}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc, val_precision, val_recall, val_f1, val_auc = self.validate_epoch(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step()
            
            # è®°å½•å†å²
            current_lr = self.optimizer.param_groups[0]['lr']
            self.train_history['epoch'].append(epoch + 1)
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_acc'].append(train_acc)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_acc'].append(val_acc)
            self.train_history['val_f1'].append(val_f1)
            self.train_history['val_auc'].append(val_auc)
            self.train_history['lr'].append(current_lr)
            
            # TensorBoardè®°å½•
            self.writer.add_scalar('Loss/Train', train_loss, epoch)
            self.writer.add_scalar('Loss/Val', val_loss, epoch)
            self.writer.add_scalar('Acc/Train', train_acc, epoch)
            self.writer.add_scalar('Acc/Val', val_acc, epoch)
            self.writer.add_scalar('F1/Val', val_f1, epoch)
            self.writer.add_scalar('AUC/Val', val_auc, epoch)
            self.writer.add_scalar('LR', current_lr, epoch)
            
            # æ˜¾ç¤ºç»“æœ
            epoch_time = time.time() - epoch_start
            print(f"ğŸ“ˆ è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.4f}")
            print(f"ğŸ“Š éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f}, AUC={val_auc:.4f}")
            print(f"â±ï¸  ç”¨æ—¶: {epoch_time:.1f}s, LR: {current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            is_best = val_acc > self.best_val_acc
            if is_best:
                self.best_val_acc = val_acc
                self.best_val_loss = val_loss
                self.best_epoch = epoch + 1
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self.save_checkpoint(epoch + 1, is_best)
            
            # æ—©åœæ£€æŸ¥
            if epoch - self.best_epoch + 1 >= self.config['training']['early_stopping']:
                print(f"ğŸ”´ æ—©åœ: {self.config['training']['early_stopping']} epochsæ— æå‡")
                break
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")
        print(f"ğŸ† æœ€ä½³ç»“æœ: Acc={self.best_val_acc:.4f} (epoch {self.best_epoch})")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = os.path.join(self.save_dir, 'training_history.json')
        with open(history_path, 'w') as f:
            json.dump(self.train_history, f, indent=2)
        
        self.writer.close()

def create_simple_config():
    """åˆ›å»ºç®€åŒ–è®­ç»ƒé…ç½®"""
    config = {
        'model': {
            'segment_length': 64,
            'num_segments': 8,
            'feature_dim': 128,
            'hidden_dim': 128
        },
        'training': {
            'epochs': 50,
            'learning_rate': 0.01,  # è¿›ä¸€æ­¥æé«˜å­¦ä¹ ç‡
            'weight_decay': 1e-5,   # é™ä½æƒé‡è¡°å‡
            'early_stopping': 15,
            'use_focal_loss': False,  # å…ˆä¸ä½¿ç”¨Focal Loss
            'steps_per_epoch': 100  # éœ€è¦æ ¹æ®å®é™…æ•°æ®è®¾ç½®
        },
        'data': {
            'batch_size': 32,
            'max_points': 1000,
            'num_workers': 4
        },
        'save_dir': f'experiments/simple_exp_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    }
    return config

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='EdgeSparkç®€åŒ–ç‰ˆè®­ç»ƒ')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--use-focal', action='store_true', help='ä½¿ç”¨Focal Loss')
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = create_simple_config()
    config['training']['epochs'] = args.epochs
    config['training']['learning_rate'] = args.lr
    config['training']['use_focal_loss'] = args.use_focal
    config['data']['batch_size'] = args.batch_size
    
    # ä¿å­˜é…ç½®
    os.makedirs(config['save_dir'], exist_ok=True)
    with open(os.path.join(config['save_dir'], 'config.json'), 'w') as f:
        json.dump(config, f, indent=2)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader = create_simple_dataloaders(
        "dataset/train_set.pkl",
        "dataset/valid_set.pkl",
        "dataset/test_set.pkl",
        batch_size=config['data']['batch_size'],
        max_points=config['data']['max_points'],
        num_workers=config['data']['num_workers']
    )
    
    # æ›´æ–°steps_per_epoch
    config['training']['steps_per_epoch'] = len(train_loader)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SimpleTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(train_loader, val_loader)
    
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {config['save_dir']}")

if __name__ == "__main__":
    main()