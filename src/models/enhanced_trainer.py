"""
å¢å¼ºç‰¹å¾æå–å™¨çš„è®­ç»ƒå™¨
ä½¿ç”¨å‡ ä½•ç‰¹å¾å’Œå¯¹æ¯”å­¦ä¹ 
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import time
import json
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'data'))
from improved_dataset_loader import create_improved_dataloaders
from enhanced_feature_extractor import EnhancedFragmentMatcher, ContrastiveLoss

class EnhancedTrainer:
    """å¢å¼ºç‰¹å¾æå–å™¨è®­ç»ƒå™¨"""
    
    def __init__(self, use_contrastive=True, contrastive_weight=0.5):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºæ¨¡å‹
        self.model = EnhancedFragmentMatcher(max_points=1000).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-4)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.5, patience=5
        )
        
        # æŸå¤±å‡½æ•°
        self.bce_loss = nn.BCEWithLogitsLoss()
        self.contrastive_loss = ContrastiveLoss(margin=0.5, temperature=0.1)
        self.use_contrastive = use_contrastive
        self.contrastive_weight = contrastive_weight
        
        # å†å²è®°å½•
        self.history = []
        
        print(f"ğŸš€ å¢å¼ºç‰¹å¾æå–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        print(f"ğŸ”§ ä½¿ç”¨å¯¹æ¯”å­¦ä¹ : {use_contrastive}")
        
    def train_epoch(self, loader, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        total_bce_loss = 0.0
        total_contrastive_loss = 0.0
        all_preds = []
        all_labels = []
        
        for batch_idx, batch in enumerate(loader):
            points1 = batch['source_points'].to(self.device)
            points2 = batch['target_points'].to(self.device)
            labels = batch['label'].to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            logits, features1, features2 = self.model(points1, points2)
            
            # BCEæŸå¤±
            bce_loss = self.bce_loss(logits, labels)
            
            # å¯¹æ¯”æŸå¤±
            contrastive_loss = 0
            if self.use_contrastive:
                contrastive_loss = self.contrastive_loss(features1, features2, labels.squeeze())
            
            # æ€»æŸå¤±
            total_batch_loss = bce_loss + self.contrastive_weight * contrastive_loss
            
            # åå‘ä¼ æ’­
            total_batch_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += total_batch_loss.item()
            total_bce_loss += bce_loss.item()
            if self.use_contrastive:
                total_contrastive_loss += contrastive_loss.item()
            
            probs = torch.sigmoid(logits)
            preds = (probs > 0.5).float()
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            if batch_idx % 20 == 0:
                if self.use_contrastive:
                    print(f'    Batch {batch_idx:3d}/{len(loader):3d} | '
                          f'Loss: {total_batch_loss.item():.4f} '
                          f'(BCE: {bce_loss.item():.4f}, Cont: {contrastive_loss.item():.4f})')
                else:
                    print(f'    Batch {batch_idx:3d}/{len(loader):3d} | Loss: {total_batch_loss.item():.4f}')
        
        avg_loss = total_loss / len(loader)
        avg_bce_loss = total_bce_loss / len(loader)
        avg_contrastive_loss = total_contrastive_loss / len(loader) if self.use_contrastive else 0
        acc = accuracy_score(all_labels, all_preds)
        
        return avg_loss, avg_bce_loss, avg_contrastive_loss, acc
    
    def validate_epoch(self, loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_probs = []
        all_labels = []
        all_features1 = []
        all_features2 = []
        
        with torch.no_grad():
            for batch in loader:
                points1 = batch['source_points'].to(self.device)
                points2 = batch['target_points'].to(self.device)
                labels = batch['label'].to(self.device)
                
                logits, features1, features2 = self.model(points1, points2)
                loss = self.bce_loss(logits, labels)
                
                total_loss += loss.item()
                probs = torch.sigmoid(logits)
                preds = (probs > 0.5).float()
                
                all_preds.extend(preds.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                all_features1.extend(features1.cpu().numpy())
                all_features2.extend(features2.cpu().numpy())
        
        avg_loss = total_loss / len(loader)
        acc = accuracy_score(all_labels, all_preds)
        
        try:
            precision, recall, f1, _ = precision_recall_fscore_support(
                all_labels, all_preds, average='binary', zero_division=0
            )
            auc = roc_auc_score(all_labels, all_probs)
        except:
            precision, recall, f1, auc = 0.0, 0.0, 0.0, 0.5
        
        # è®¡ç®—ç‰¹å¾å¯åˆ†ç¦»æ€§
        separability = self.compute_feature_separability(all_features1, all_features2, all_labels)
        
        return avg_loss, acc, precision, recall, f1, auc, separability, all_features1, all_features2, all_labels
    
    def compute_feature_separability(self, features1, features2, labels):
        """è®¡ç®—ç‰¹å¾å¯åˆ†ç¦»æ€§"""
        features1 = np.array(features1)
        features2 = np.array(features2)
        labels = np.array(labels).flatten()
        
        # ç»„åˆç‰¹å¾ (ç®€å•æ‹¼æ¥)
        combined_features = np.concatenate([features1, features2], axis=1)
        
        # æ­£è´Ÿæ ·æœ¬åˆ†ç¦»
        pos_features = combined_features[labels == 1]
        neg_features = combined_features[labels == 0]
        
        if len(pos_features) < 2 or len(neg_features) < 2:
            return 0.0
        
        # è®¡ç®—ç±»é—´è·ç¦»
        pos_center = np.mean(pos_features, axis=0)
        neg_center = np.mean(neg_features, axis=0)
        inter_class_dist = np.linalg.norm(pos_center - neg_center)
        
        # è®¡ç®—ç±»å†…è·ç¦»
        pos_distances = [np.linalg.norm(f - pos_center) for f in pos_features]
        neg_distances = [np.linalg.norm(f - neg_center) for f in neg_features]
        avg_intra_class_dist = (np.mean(pos_distances) + np.mean(neg_distances)) / 2
        
        # å¯åˆ†ç¦»æ€§æ¯”ç‡
        separability = inter_class_dist / (avg_intra_class_dist + 1e-8)
        
        return separability
    
    def visualize_features(self, features1, features2, labels, epoch, save_dir="feature_visualizations"):
        """å¯è§†åŒ–ç‰¹å¾ç©ºé—´"""
        os.makedirs(save_dir, exist_ok=True)
        
        # ç»„åˆç‰¹å¾
        combined_features = np.concatenate([features1, features2], axis=1)
        labels = np.array(labels).flatten()
        
        # ä½¿ç”¨t-SNEé™ç»´
        if len(combined_features) > 50:  # åªæœ‰è¶³å¤Ÿçš„æ ·æœ¬æ‰è¿›è¡Œå¯è§†åŒ–
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(combined_features)//4))
            features_2d = tsne.fit_transform(combined_features)
            
            # ç»˜åˆ¶
            plt.figure(figsize=(10, 8))
            colors = ['red' if label == 0 else 'blue' for label in labels]
            plt.scatter(features_2d[:, 0], features_2d[:, 1], c=colors, alpha=0.6)
            plt.title(f'Feature Space Visualization - Epoch {epoch}')
            plt.xlabel('t-SNE Dimension 1')
            plt.ylabel('t-SNE Dimension 2')
            plt.legend(['Non-matching', 'Matching'])
            plt.grid(True, alpha=0.3)
            
            # ä¿å­˜
            plt.savefig(f'{save_dir}/features_epoch_{epoch}.png', dpi=150, bbox_inches='tight')
            plt.close()
    
    def train(self, train_loader, val_loader, epochs=30):
        """è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºç‰¹å¾æå–å™¨")
        
        best_separability = 0.0
        best_acc = 0.0
        
        for epoch in range(epochs):
            print(f"\nğŸ“Š Epoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss, train_bce, train_cont, train_acc = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_results = self.validate_epoch(val_loader)
            val_loss, val_acc, val_precision, val_recall, val_f1, val_auc, separability = val_results[:7]
            val_features1, val_features2, val_labels = val_results[7:]
            
            # å­¦ä¹ ç‡æ›´æ–°
            self.scheduler.step(val_acc)
            
            # è®°å½•
            epoch_record = {
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'train_bce_loss': train_bce,
                'train_contrastive_loss': train_cont,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc,
                'val_f1': val_f1,
                'val_auc': val_auc,
                'separability': separability,
                'lr': self.optimizer.param_groups[0]['lr']
            }
            self.history.append(epoch_record)
            
            # æ˜¾ç¤ºç»“æœ
            print(f"ğŸ“ˆ è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.4f}")
            if self.use_contrastive:
                print(f"   (BCE: {train_bce:.4f}, Contrastive: {train_cont:.4f})")
            print(f"ğŸ“Š éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f}, AUC={val_auc:.4f}")
            print(f"ğŸ¯ ç‰¹å¾å¯åˆ†ç¦»æ€§: {separability:.4f}")
            print(f"ğŸ“š å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # å¯è§†åŒ–ç‰¹å¾ç©ºé—´
            if epoch % 5 == 0:
                self.visualize_features(val_features1, val_features2, val_labels, epoch)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if separability > best_separability:
                best_separability = separability
                print(f"  ğŸ’¾ æ–°çš„æœ€ä½³å¯åˆ†ç¦»æ€§: {best_separability:.4f}")
                torch.save(self.model.state_dict(), 'best_enhanced_model_separability.pth')
            
            if val_acc > best_acc:
                best_acc = val_acc
                print(f"  ğŸ’¾ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
                torch.save(self.model.state_dict(), 'best_enhanced_model_accuracy.pth')
            
            # æå‰åœæ­¢æ£€æŸ¥
            if epoch >= 10 and separability < 0.1:
                print("ğŸ”´ ç‰¹å¾å¯åˆ†ç¦»æ€§è¿‡ä½ï¼Œæå‰åœæ­¢")
                break
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³å¯åˆ†ç¦»æ€§: {best_separability:.4f}")
        print(f"æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
        
        return best_separability, best_acc

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¥ å¢å¼ºç‰¹å¾æå–å™¨è®­ç»ƒ")
    print("=" * 50)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader = create_improved_dataloaders(
        "dataset/train_set.pkl",
        "dataset/valid_set.pkl",
        "dataset/test_set.pkl",
        batch_size=16,  # å‡å°batch sizeé¿å…æ˜¾å­˜ä¸è¶³
        max_points=1000,
        num_workers=2,
        sampling_strategy='ordered'
    )
    
    # å¯¹æ¯”å®éªŒï¼šæ™®é€šç‰ˆæœ¬ vs å¯¹æ¯”å­¦ä¹ ç‰ˆæœ¬
    experiments = [
        {"name": "å¢å¼ºç‰¹å¾ (BCE only)", "use_contrastive": False},
        {"name": "å¢å¼ºç‰¹å¾ + å¯¹æ¯”å­¦ä¹ ", "use_contrastive": True, "contrastive_weight": 0.3},
        {"name": "å¢å¼ºç‰¹å¾ + å¼ºå¯¹æ¯”å­¦ä¹ ", "use_contrastive": True, "contrastive_weight": 0.7}
    ]
    
    results = {}
    
    for exp in experiments:
        print(f"\nğŸ”¬ å®éªŒ: {exp['name']}")
        print("=" * 60)
        
        try:
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = EnhancedTrainer(
                use_contrastive=exp["use_contrastive"],
                contrastive_weight=exp.get("contrastive_weight", 0.5)
            )
            
            # è®­ç»ƒ
            best_sep, best_acc = trainer.train(train_loader, val_loader, epochs=20)
            
            results[exp["name"]] = {
                "best_separability": best_sep,
                "best_accuracy": best_acc,
                "final_epoch": len(trainer.history),
                "history": trainer.history
            }
            
            print(f"ğŸ“Š {exp['name']} ç»“æœ:")
            print(f"  æœ€ä½³å¯åˆ†ç¦»æ€§: {best_sep:.4f}")
            print(f"  æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
            
        except Exception as e:
            print(f"âŒ {exp['name']} å®éªŒå¤±è´¥: {e}")
            results[exp["name"]] = {"error": str(e)}
    
    # ç»“æœå¯¹æ¯”
    print(f"\n" + "=" * 70)
    print("ğŸ“ˆ å®éªŒç»“æœå¯¹æ¯”")
    print("=" * 70)
    
    best_exp = None
    best_separability = 0.0
    
    for name, result in results.items():
        if "error" not in result:
            sep = result["best_separability"]
            acc = result["best_accuracy"]
            print(f"{name}:")
            print(f"  å¯åˆ†ç¦»æ€§: {sep:.4f}")
            print(f"  å‡†ç¡®ç‡: {acc:.4f}")
            
            if sep > best_separability:
                best_separability = sep
                best_exp = name
        else:
            print(f"{name}: å¤±è´¥ ({result['error']})")
    
    if best_exp:
        print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_exp}")
        print(f"ğŸ¯ æœ€ä½³å¯åˆ†ç¦»æ€§: {best_separability:.4f}")
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"enhanced_feature_results_{timestamp}.json"
    
    # ç®€åŒ–ç»“æœç”¨äºä¿å­˜
    simplified_results = {}
    for name, result in results.items():
        if "history" in result:
            simplified_results[name] = {
                "best_separability": result["best_separability"],
                "best_accuracy": result["best_accuracy"],
                "final_epoch": result["final_epoch"]
            }
        else:
            simplified_results[name] = result
    
    with open(results_file, 'w') as f:
        json.dump(simplified_results, f, indent=2)
    
    print(f"ğŸ“ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    return results

if __name__ == "__main__":
    results = main()