"""
æ”¹è¿›çš„EdgeSparkæ–¹æ³•ï¼šå¼•å…¥PairingNetçš„å…³é”®æŠ€æœ¯
1. FocalLosså¤„ç†æ ·æœ¬ä¸å¹³è¡¡
2. æ¸©åº¦ç¼©æ”¾çš„ç›¸ä¼¼åº¦è®¡ç®—
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import os
import time
import json
import math
from datetime import datetime
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'data'))
from improved_dataset_loader import create_improved_dataloaders

class FocalLoss(nn.Module):
    """
    FocalLoss: ä¸“é—¨å¤„ç†æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜
    åŸºäºPairingNetçš„å®ç°ï¼Œä½†ç®€åŒ–ä¸ºäºŒåˆ†ç±»ç‰ˆæœ¬
    """
    def __init__(self, alpha=0.55, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        """
        Args:
            inputs: é¢„æµ‹logits (batch_size, 1)
            targets: çœŸå®æ ‡ç­¾ (batch_size, 1)
        """
        # è®¡ç®—BCEæŸå¤±
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        
        # è®¡ç®—æ¦‚ç‡
        p_t = torch.exp(-bce_loss)
        
        # è®¡ç®—alphaæƒé‡
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)
        
        # è®¡ç®—focalæƒé‡
        focal_weight = alpha_t * (1 - p_t) ** self.gamma
        
        # åº”ç”¨focalæƒé‡
        focal_loss = focal_weight * bce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class EdgeShapeEncoder(nn.Module):
    """
    è¾¹ç¼˜å½¢çŠ¶ç¼–ç å™¨ - ä¿æŒåŸæœ‰è®¾è®¡
    """
    def __init__(self, max_points=1000):
        super().__init__()
        self.max_points = max_points
        
        # 1. å±€éƒ¨å½¢çŠ¶ç‰¹å¾æå–
        self.local_conv = nn.Sequential(
            nn.Conv1d(2, 64, kernel_size=9, padding=4),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Conv1d(64, 128, kernel_size=7, padding=3),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Conv1d(128, 256, kernel_size=5, padding=2),
            nn.BatchNorm1d(256),
            nn.ReLU(),
        )
        
        # 2. å…¨å±€å½¢çŠ¶ç‰¹å¾æå–
        self.global_conv = nn.Sequential(
            nn.Conv1d(256, 512, kernel_size=3, padding=1),
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Conv1d(512, 256, kernel_size=3, padding=1),
            nn.BatchNorm1d(256),
            nn.ReLU(),
        )
        
        # 3. è‡ªé€‚åº”æ± åŒ–
        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)
        
        # 4. æœ€ç»ˆæŠ•å½±
        self.final_proj = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64)
        )
        
    def forward(self, points):
        """
        ç¼–ç è¾¹ç¼˜å½¢çŠ¶
        Args:
            points: (batch_size, num_points, 2)
        Returns:
            features: (batch_size, 64)
        """
        # è½¬æ¢ç»´åº¦
        x = points.transpose(1, 2)  # (batch_size, 2, num_points)
        
        # å±€éƒ¨ç‰¹å¾æå–
        local_features = self.local_conv(x)  # (batch_size, 256, num_points)
        
        # å…¨å±€ç‰¹å¾æå–
        global_features = self.global_conv(local_features)  # (batch_size, 256, num_points)
        
        # è‡ªé€‚åº”æ± åŒ–
        pooled = self.adaptive_pool(global_features).squeeze(-1)  # (batch_size, 256)
        
        # æœ€ç»ˆæŠ•å½±
        final_features = self.final_proj(pooled)  # (batch_size, 64)
        
        return final_features

class ImprovedEdgeMatchingNet(nn.Module):
    """
    æ”¹è¿›çš„è¾¹ç¼˜åŒ¹é…ç½‘ç»œï¼šå¼•å…¥æ¸©åº¦ç¼©æ”¾
    """
    def __init__(self, max_points=1000, temperature=1.0):
        super().__init__()
        self.max_points = max_points
        self.temperature = temperature
        
        # å½¢çŠ¶ç¼–ç å™¨
        self.shape_encoder = EdgeShapeEncoder(max_points)
        
        # åŒ¹é…ç½‘ç»œ
        self.matching_net = nn.Sequential(
            # è¾“å…¥ï¼šä¸¤ä¸ª64ç»´ç‰¹å¾æ‹¼æ¥ + å·®å€¼ + æ¸©åº¦ç¼©æ”¾çš„ç›¸ä¼¼åº¦
            nn.Linear(64 * 2 + 64 + 1, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 1)
        )
        
    def get_temperature_scaled_similarity(self, features1, features2):
        """
        è®¡ç®—æ¸©åº¦ç¼©æ”¾çš„ç›¸ä¼¼åº¦
        Args:
            features1: (batch_size, 64)
            features2: (batch_size, 64)
        Returns:
            scaled_similarity: (batch_size, 1)
        """
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        features1_norm = F.normalize(features1, p=2, dim=1)
        features2_norm = F.normalize(features2, p=2, dim=1)
        
        # ç‚¹ç§¯ç›¸ä¼¼åº¦
        dot_similarity = torch.sum(features1_norm * features2_norm, dim=1, keepdim=True)
        
        # æ¸©åº¦ç¼©æ”¾
        scaled_similarity = dot_similarity / self.temperature
        
        return scaled_similarity
        
    def forward(self, points1, points2):
        """
        å‰å‘ä¼ æ’­
        Args:
            points1: (batch_size, num_points1, 2)
            points2: (batch_size, num_points2, 2)
        Returns:
            match_logits: (batch_size, 1)
        """
        # å½¢çŠ¶ç¼–ç 
        shape1 = self.shape_encoder(points1)  # (batch_size, 64)
        shape2 = self.shape_encoder(points2)  # (batch_size, 64)
        
        # ç‰¹å¾ç»„åˆ
        diff = shape1 - shape2  # å·®å€¼ç‰¹å¾
        temp_sim = self.get_temperature_scaled_similarity(shape1, shape2)  # æ¸©åº¦ç¼©æ”¾ç›¸ä¼¼åº¦
        
        # æ‹¼æ¥æ‰€æœ‰ç‰¹å¾
        combined = torch.cat([shape1, shape2, diff, temp_sim], dim=1)  # (batch_size, 64*2+64+1)
        
        # åŒ¹é…é¢„æµ‹
        match_logits = self.matching_net(combined)
        
        return match_logits

class ImprovedFinalTrainer:
    """æ”¹è¿›çš„æœ€ç»ˆè®­ç»ƒå™¨ï¼šä½¿ç”¨FocalLosså’Œæ¸©åº¦ç¼©æ”¾"""
    
    def __init__(self, temperature=1.0, alpha=0.55, gamma=2.0):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.temperature = temperature
        
        # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
        self.model = ImprovedEdgeMatchingNet(
            max_points=1000, 
            temperature=temperature
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-4)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=15, gamma=0.5)
        
        # ä½¿ç”¨FocalLoss
        self.criterion = FocalLoss(alpha=alpha, gamma=gamma)
        
        # å†å²è®°å½•
        self.history = []
        
        print(f"âœ… æ”¹è¿›è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"   - æ¸©åº¦ç¼©æ”¾: {temperature}")
        print(f"   - FocalLoss: Î±={alpha}, Î³={gamma}")
        print(f"   - æ¨¡å‹å‚æ•°: {sum(p.numel() for p in self.model.parameters()):,}")
        
    def train_epoch(self, loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_preds = []
        all_labels = []
        
        for batch_idx, batch in enumerate(loader):
            points1 = batch['source_points'].to(self.device)
            points2 = batch['target_points'].to(self.device)
            labels = batch['label'].to(self.device)
            
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            logits = self.model(points1, points2)
            
            # ä½¿ç”¨FocalLoss
            loss = self.criterion(logits, labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            probs = torch.sigmoid(logits)
            preds = (probs > 0.5).float()
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            if batch_idx % 20 == 0:
                print(f'    Batch {batch_idx:3d}/{len(loader):3d} | Loss: {loss.item():.4f}')
        
        avg_loss = total_loss / len(loader)
        acc = accuracy_score(all_labels, all_preds)
        
        return avg_loss, acc
    
    def validate_epoch(self, loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        total_loss = 0.0
        all_preds = []
        all_probs = []
        all_labels = []
        
        with torch.no_grad():
            for batch in loader:
                points1 = batch['source_points'].to(self.device)
                points2 = batch['target_points'].to(self.device)
                labels = batch['label'].to(self.device)
                
                logits = self.model(points1, points2)
                loss = self.criterion(logits, labels)
                
                total_loss += loss.item()
                probs = torch.sigmoid(logits)
                preds = (probs > 0.5).float()
                
                all_preds.extend(preds.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        avg_loss = total_loss / len(loader)
        acc = accuracy_score(all_labels, all_preds)
        
        try:
            precision, recall, f1, _ = precision_recall_fscore_support(
                all_labels, all_preds, average='binary', zero_division=0
            )
            auc = roc_auc_score(all_labels, all_probs)
        except:
            precision, recall, f1, auc = 0.0, 0.0, 0.0, 0.5
        
        return avg_loss, acc, precision, recall, f1, auc
    
    def train(self, train_loader, val_loader, epochs=30):
        """è®­ç»ƒå¾ªç¯"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ”¹è¿›ç‰ˆæœ¬ (FocalLoss + æ¸©åº¦ç¼©æ”¾)")
        
        best_acc = 0.0
        
        for epoch in range(epochs):
            print(f"\nğŸ“Š Epoch {epoch+1}/{epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc, val_precision, val_recall, val_f1, val_auc = self.validate_epoch(val_loader)
            
            # å­¦ä¹ ç‡æ›´æ–°
            self.scheduler.step()
            
            # è®°å½•
            self.history.append({
                'epoch': epoch + 1,
                'train_loss': train_loss,
                'train_acc': train_acc,
                'val_loss': val_loss,
                'val_acc': val_acc,
                'val_f1': val_f1,
                'val_auc': val_auc,
                'lr': self.optimizer.param_groups[0]['lr']
            })
            
            # æ˜¾ç¤ºç»“æœ
            print(f"ğŸ“ˆ è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.4f}")
            print(f"ğŸ“Š éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f}, AUC={val_auc:.4f}")
            print(f"ğŸ“š å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_acc:
                best_acc = val_acc
                print(f"  ğŸ’¾ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
                
                # ä¿å­˜æ¨¡å‹
                torch.save(self.model.state_dict(), 'best_improved_model.pth')
            
            # æå‰åœæ­¢æ£€æŸ¥
            if epoch >= 10 and val_acc < 0.52:
                print("ğŸ”´ éªŒè¯å‡†ç¡®ç‡è¿‡ä½ï¼Œæå‰åœæ­¢")
                break
        
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_acc:.4f}")
        return best_acc

def run_improved_experiment(sampling_strategy='ordered', temperature=1.0, alpha=0.55, gamma=2.0, epochs=25):
    """è¿è¡Œæ”¹è¿›å®éªŒ"""
    print(f"\nğŸ”¬ æ”¹è¿›å®éªŒ: ç­–ç•¥={sampling_strategy}, T={temperature}, Î±={alpha}, Î³={gamma}")
    print("=" * 70)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader = create_improved_dataloaders(
        "dataset/train_set.pkl",
        "dataset/valid_set.pkl", 
        "dataset/test_set.pkl",
        batch_size=32,
        max_points=1000,
        num_workers=4,
        sampling_strategy=sampling_strategy
    )
    
    # åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒå™¨
    trainer = ImprovedFinalTrainer(temperature=temperature, alpha=alpha, gamma=gamma)
    
    # å¼€å§‹è®­ç»ƒ
    best_acc = trainer.train(train_loader, val_loader, epochs=epochs)
    
    # ä¿å­˜æ¨¡å‹
    model_name = f'best_improved_model_{sampling_strategy}_T{temperature}_a{alpha}_g{gamma}.pth'
    if os.path.exists('best_improved_model.pth'):
        os.rename('best_improved_model.pth', model_name)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º: {model_name}")
    
    return best_acc, trainer.history

def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ”¹è¿›å®éªŒ"""
    print("ğŸ”¥ EdgeSparkæ”¹è¿›å®éªŒ - FocalLoss + æ¸©åº¦ç¼©æ”¾")
    print("=" * 70)
    
    # å®éªŒé…ç½®
    experiments = [
        {'strategy': 'ordered', 'temperature': 1.0, 'alpha': 0.55, 'gamma': 2.0},
        {'strategy': 'ordered', 'temperature': 0.5, 'alpha': 0.55, 'gamma': 2.0},
        {'strategy': 'ordered', 'temperature': 2.0, 'alpha': 0.55, 'gamma': 2.0},
        {'strategy': 'ordered', 'temperature': 1.0, 'alpha': 0.25, 'gamma': 2.0},
        {'strategy': 'ordered', 'temperature': 1.0, 'alpha': 0.75, 'gamma': 2.0},
    ]
    
    results = {}
    
    # è¿è¡Œæ¯ä¸ªå®éªŒ
    for i, exp in enumerate(experiments):
        try:
            print(f"\nğŸ”¬ å®éªŒ {i+1}/{len(experiments)}")
            
            best_acc, history = run_improved_experiment(
                sampling_strategy=exp['strategy'],
                temperature=exp['temperature'],
                alpha=exp['alpha'],
                gamma=exp['gamma'],
                epochs=25
            )
            
            exp_name = f"{exp['strategy']}_T{exp['temperature']}_a{exp['alpha']}_g{exp['gamma']}"
            results[exp_name] = {
                'best_acc': best_acc,
                'history': history,
                'params': exp
            }
            
            print(f"âœ… å®éªŒå®Œæˆ: {exp_name} = {best_acc:.4f}")
            
        except Exception as e:
            print(f"âŒ å®éªŒå¤±è´¥: {e}")
            exp_name = f"{exp['strategy']}_T{exp['temperature']}_a{exp['alpha']}_g{exp['gamma']}"
            results[exp_name] = {'best_acc': 0.0, 'error': str(e)}
    
    # ç»“æœåˆ†æ
    print(f"\n" + "=" * 70)
    print("ğŸ“ˆ æ”¹è¿›å®éªŒç»“æœ")
    print("=" * 70)
    
    best_exp = None
    best_score = 0.0
    
    for exp_name, result in results.items():
        if 'error' not in result:
            acc = result['best_acc']
            params = result['params']
            print(f"{exp_name:>25}: {acc:.4f} (T={params['temperature']}, Î±={params['alpha']}, Î³={params['gamma']})")
            if acc > best_score:
                best_score = acc
                best_exp = exp_name
        else:
            print(f"{exp_name:>25}: å¤±è´¥ ({result['error']})")
    
    print(f"\nğŸ† æœ€ä½³å®éªŒ: {best_exp} (å‡†ç¡®ç‡: {best_score:.4f})")
    
    # ä¿å­˜å®éªŒç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"improved_experiment_results_{timestamp}.json"
    
    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
    serializable_results = {}
    for exp_name, result in results.items():
        if 'history' in result:
            serializable_results[exp_name] = {
                'best_acc': result['best_acc'],
                'params': result['params'],
                'final_epoch': len(result['history']),
                'final_val_acc': result['history'][-1]['val_acc'] if result['history'] else 0.0
            }
        else:
            serializable_results[exp_name] = result
    
    with open(results_file, 'w') as f:
        json.dump(serializable_results, f, indent=2)
    
    print(f"ğŸ“ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # æ”¹è¿›æ•ˆæœåˆ†æ
    print(f"\n=== æ”¹è¿›æ•ˆæœåˆ†æ ===")
    if best_score > 0.8:
        print("ğŸ‰ æ˜¾è‘—æ”¹è¿›ï¼FocalLoss + æ¸©åº¦ç¼©æ”¾æ•ˆæœæ˜¾è‘—")
    elif best_score > 0.7:
        print("âœ… æ˜æ˜¾æ”¹è¿›ï¼æŠ€æœ¯æœ‰æ•ˆ")
    elif best_score > 0.6:
        print("âš ï¸  å°å¹…æ”¹è¿›ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("âŒ æ”¹è¿›æ•ˆæœä¸æ˜æ˜¾")
        print("ğŸ’¡ å»ºè®®ï¼š")
        print("   1. è°ƒæ•´æ¸©åº¦å‚æ•°èŒƒå›´")
        print("   2. ä¼˜åŒ–FocalLosså‚æ•°")
        print("   3. æ”¹è¿›ç‰¹å¾ç¼–ç å™¨")
        print("   4. å¢åŠ æ•°æ®å¢å¼º")
    
    return best_score

if __name__ == "__main__":
    result = main()