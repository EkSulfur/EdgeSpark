"""
EdgeSparkç‰¹å¾å¯è§†åŒ–å·¥å…·
åŸºäºnetwork_improved.pyçš„ç‰¹å¾æå–å’Œt-SNEå¯è§†åŒ–
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import os
import json
from datetime import datetime

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from models.network_improved import EdgeSparkNet
from data.improved_dataset_loader import create_dataloaders

class FeatureVisualizer:
    """EdgeSparkç‰¹å¾å¯è§†åŒ–å™¨"""
    
    def __init__(self, model_path=None, device=None):
        """
        åˆå§‹åŒ–ç‰¹å¾å¯è§†åŒ–å™¨
        Args:
            model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆ›å»ºæ¨¡å‹
        self.model = EdgeSparkNet(
            segment_length=32,
            n1=160,
            n2=160,
            feature_dim=256,
            hidden_channels=64,
            temperature=1.0,
            num_samples=1
        ).to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        if model_path and os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… å·²åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
            else:
                self.model.load_state_dict(checkpoint)
                print(f"âœ… å·²åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        else:
            print("âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        
        self.model.eval()
        
    def extract_features(self, data_loader, max_samples=500):
        """
        Extract features
        Args:
            data_loader: Data loader
            max_samples: Maximum number of samples
        Returns:
            features: Extracted features
            labels: Corresponding labels
        """
        print("ğŸ” Starting feature extraction...")
        
        all_features = []
        all_labels = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(data_loader):
                if len(all_features) >= max_samples:
                    break
                
                source_points = batch['source_points'].to(self.device)
                target_points = batch['target_points'].to(self.device)
                labels = batch['label'].to(self.device)
                
                # æå–ä¸­é—´ç‰¹å¾
                features = self._extract_intermediate_features(source_points, target_points)
                
                all_features.append(features.cpu().numpy())
                all_labels.append(labels.cpu().numpy())
                
                if batch_idx % 10 == 0:
                    print(f"  Processing batch {batch_idx+1}/{len(data_loader)}")
        
        # Merge all features
        features = np.vstack(all_features)
        labels = np.vstack(all_labels).flatten()
        
        print(f"âœ… Feature extraction completed: {features.shape[0]} samples, {features.shape[1]} features")
        return features, labels
    
    def _extract_intermediate_features(self, source_points, target_points):
        """
        æå–ä¸­é—´å±‚ç‰¹å¾
        """
        batch_size = source_points.shape[0]
        
        # 1. æ®µé‡‡æ ·
        segments1_list = []
        segments2_list = []
        
        for i in range(batch_size):
            seg1 = self.model.segment_sampler(source_points[i], self.model.n1)
            seg2 = self.model.segment_sampler(target_points[i], self.model.n2)
            segments1_list.append(seg1)
            segments2_list.append(seg2)
        
        segments1 = torch.stack(segments1_list)
        segments2 = torch.stack(segments2_list)
        
        # 2. ç‰¹å¾ç¼–ç 
        features1 = self.model.segment_encoder(segments1)
        features2 = self.model.segment_encoder(segments2)
        
        # 3. äº¤å‰æ³¨æ„åŠ›å¢å¼º
        enhanced_features1, enhanced_features2 = self.model.cross_attention(features1, features2)
        
        # 4. ç›¸ä¼¼åº¦è®¡ç®—
        similarity_matrix = self.model.similarity_computer(enhanced_features1, enhanced_features2)
        
        # 5. ç‰¹å¾å¤„ç†
        processed_features = self.model.similarity_processor(similarity_matrix)
        attention_features = self.model.attention_pooling(similarity_matrix)
        
        # 6. ç‰¹å¾èåˆ
        fused_features = processed_features + attention_features
        
        return fused_features
    
    def visualize_features(self, features, labels, save_dir="feature_visualizations", epoch=None):
        """
        å¯è§†åŒ–ç‰¹å¾ç©ºé—´
        Args:
            features: ç‰¹å¾çŸ©é˜µ (n_samples, n_features)
            labels: æ ‡ç­¾ (n_samples,)
            save_dir: ä¿å­˜ç›®å½•
            epoch: å½“å‰epoch (å¯é€‰)
        """
        os.makedirs(save_dir, exist_ok=True)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        # è®¾ç½®ç»˜å›¾æ ·å¼
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        # è®¾ç½®å­—ä½“ä»¥é¿å…ä¸­æ–‡ä¹±ç 
        plt.rcParams['font.family'] = 'DejaVu Sans'
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. t-SNEå¯è§†åŒ–
        self._plot_tsne(features_scaled, labels, save_dir, epoch)
        
        # 2. PCAå¯è§†åŒ–
        self._plot_pca(features_scaled, labels, save_dir, epoch)
        
        # 3. ç‰¹å¾åˆ†å¸ƒåˆ†æ
        self._plot_feature_distribution(features_scaled, labels, save_dir, epoch)
        
        # 4. ç‰¹å¾ç›¸å…³æ€§åˆ†æ
        self._plot_feature_correlation(features_scaled, labels, save_dir, epoch)
        
        print(f"âœ… Feature visualization completed, saved to: {save_dir}")
    
    def _plot_tsne(self, features, labels, save_dir, epoch):
        """t-SNEå¯è§†åŒ–"""
        print("ğŸ¨ Generating t-SNE visualization...")
        
        # æ£€æŸ¥æ ·æœ¬æ•°é‡
        n_samples = len(features)
        perplexity = min(30, n_samples // 4)
        
        if n_samples < 10:
            print("  âš ï¸  Too few samples, skipping t-SNE visualization")
            return
        
        # t-SNEé™ç»´
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, max_iter=1000)
        features_2d = tsne.fit_transform(features)
        
        # ç»˜åˆ¶
        plt.figure(figsize=(12, 10))
        
        # åˆ†åˆ«ç»˜åˆ¶æ­£è´Ÿæ ·æœ¬
        pos_mask = labels == 1
        neg_mask = labels == 0
        
        plt.scatter(features_2d[neg_mask, 0], features_2d[neg_mask, 1], 
                   c='red', alpha=0.6, s=50, label='Non-matching', marker='o')
        plt.scatter(features_2d[pos_mask, 0], features_2d[pos_mask, 1], 
                   c='blue', alpha=0.6, s=50, label='Matching', marker='s')
        
        plt.title(f'Feature Space t-SNE Visualization' + (f' - Epoch {epoch}' if epoch else ''))
        plt.xlabel('t-SNE Dimension 1')
        plt.ylabel('t-SNE Dimension 2')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        pos_count = np.sum(pos_mask)
        neg_count = np.sum(neg_mask)
        plt.text(0.02, 0.98, f'Matching: {pos_count}\nNon-matching: {neg_count}', 
                transform=plt.gca().transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # ä¿å­˜
        filename = f'features_epoch_{epoch}.png' if epoch else 'features_tsne.png'
        plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_pca(self, features, labels, save_dir, epoch):
        """PCAå¯è§†åŒ–"""
        print("ğŸ¨ Generating PCA visualization...")
        
        # PCAé™ç»´
        pca = PCA(n_components=2)
        features_2d = pca.fit_transform(features)
        
        plt.figure(figsize=(12, 10))
        
        # åˆ†åˆ«ç»˜åˆ¶æ­£è´Ÿæ ·æœ¬
        pos_mask = labels == 1
        neg_mask = labels == 0
        
        plt.scatter(features_2d[neg_mask, 0], features_2d[neg_mask, 1], 
                   c='red', alpha=0.6, s=50, label='Non-matching', marker='o')
        plt.scatter(features_2d[pos_mask, 0], features_2d[pos_mask, 1], 
                   c='blue', alpha=0.6, s=50, label='Matching', marker='s')
        
        plt.title(f'Feature Space PCA Visualization' + (f' - Epoch {epoch}' if epoch else ''))
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ä¿å­˜
        filename = f'features_pca_epoch_{epoch}.png' if epoch else 'features_pca.png'
        plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_feature_distribution(self, features, labels, save_dir, epoch):
        """ç‰¹å¾åˆ†å¸ƒåˆ†æ"""
        print("ğŸ¨ Generating feature distribution analysis...")
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„åˆ†ç¦»åº¦
        separability_scores = []
        
        for i in range(features.shape[1]):
            pos_features = features[labels == 1, i]
            neg_features = features[labels == 0, i]
            
            if len(pos_features) > 0 and len(neg_features) > 0:
                # è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„åˆ†ç¦»åº¦
                pos_mean = np.mean(pos_features)
                neg_mean = np.mean(neg_features)
                pos_std = np.std(pos_features)
                neg_std = np.std(neg_features)
                
                # ä½¿ç”¨Cohen's dè®¡ç®—åˆ†ç¦»åº¦
                pooled_std = np.sqrt(((len(pos_features) - 1) * pos_std**2 + 
                                     (len(neg_features) - 1) * neg_std**2) / 
                                    (len(pos_features) + len(neg_features) - 2))
                
                if pooled_std > 0:
                    separability = abs(pos_mean - neg_mean) / pooled_std
                else:
                    separability = 0
                
                separability_scores.append(separability)
            else:
                separability_scores.append(0)
        
        # ç»˜åˆ¶åˆ†ç¦»åº¦åˆ†å¸ƒ
        plt.figure(figsize=(12, 8))
        plt.hist(separability_scores, bins=30, alpha=0.7, edgecolor='black')
        plt.title(f'Feature Separability Distribution' + (f' - Epoch {epoch}' if epoch else ''))
        plt.xlabel('Separability Score (Cohen\'s d)')
        plt.ylabel('Number of Features')
        plt.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        mean_sep = np.mean(separability_scores)
        max_sep = np.max(separability_scores)
        plt.axvline(mean_sep, color='red', linestyle='--', label=f'Mean: {mean_sep:.3f}')
        plt.axvline(max_sep, color='green', linestyle='--', label=f'Max: {max_sep:.3f}')
        plt.legend()
        
        # ä¿å­˜
        filename = f'feature_separability_epoch_{epoch}.png' if epoch else 'feature_separability.png'
        plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')
        plt.close()
    
    def _plot_feature_correlation(self, features, labels, save_dir, epoch):
        """ç‰¹å¾ç›¸å…³æ€§åˆ†æ"""
        print("ğŸ¨ Generating feature correlation analysis...")
        
        # è®¡ç®—ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ
        correlation_matrix = np.corrcoef(features.T)
        
        # ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾
        plt.figure(figsize=(10, 8))
        
        # åªæ˜¾ç¤ºéƒ¨åˆ†ç‰¹å¾ä»¥é¿å…è¿‡äºå¯†é›†
        max_features = min(50, features.shape[1])
        correlation_subset = correlation_matrix[:max_features, :max_features]
        
        sns.heatmap(correlation_subset, 
                   cmap='coolwarm', 
                   center=0,
                   square=True,
                   cbar_kws={'label': 'Correlation'})
        
        plt.title(f'Feature Correlation Matrix' + (f' - Epoch {epoch}' if epoch else ''))
        plt.xlabel('Feature Index')
        plt.ylabel('Feature Index')
        
        # ä¿å­˜
        filename = f'feature_correlation_epoch_{epoch}.png' if epoch else 'feature_correlation.png'
        plt.savefig(os.path.join(save_dir, filename), dpi=150, bbox_inches='tight')
        plt.close()
    
    def analyze_features(self, features, labels):
        """
        Analyze feature quality
        Args:
            features: Feature matrix
            labels: Labels
        Returns:
            analysis_results: Analysis results dictionary
        """
        print("ğŸ” Analyzing feature quality...")
        
        pos_features = features[labels == 1]
        neg_features = features[labels == 0]
        
        # è®¡ç®—åŸºç¡€ç»Ÿè®¡
        pos_mean = np.mean(pos_features, axis=0)
        neg_mean = np.mean(neg_features, axis=0)
        pos_std = np.std(pos_features, axis=0)
        neg_std = np.std(neg_features, axis=0)
        
        # è®¡ç®—ç±»é—´è·ç¦»
        inter_class_distance = np.linalg.norm(pos_mean - neg_mean)
        
        # è®¡ç®—ç±»å†…è·ç¦»
        pos_intra_dist = np.mean([np.linalg.norm(f - pos_mean) for f in pos_features])
        neg_intra_dist = np.mean([np.linalg.norm(f - neg_mean) for f in neg_features])
        avg_intra_dist = (pos_intra_dist + neg_intra_dist) / 2
        
        # å¯åˆ†ç¦»æ€§æ¯”ç‡
        separability_ratio = inter_class_distance / (avg_intra_dist + 1e-8)
        
        # ç‰¹å¾æ–¹å·®åˆ†æ
        feature_variances = np.var(features, axis=0)
        active_features = np.sum(feature_variances > 1e-6)
        
        analysis_results = {
            'num_samples': len(features),
            'num_features': features.shape[1],
            'num_positive': len(pos_features),
            'num_negative': len(neg_features),
            'inter_class_distance': float(inter_class_distance),
            'avg_intra_class_distance': float(avg_intra_dist),
            'separability_ratio': float(separability_ratio),
            'active_features': int(active_features),
            'feature_variance_mean': float(np.mean(feature_variances)),
            'feature_variance_std': float(np.std(feature_variances)),
            'pos_feature_mean': float(np.mean(pos_mean)),
            'neg_feature_mean': float(np.mean(neg_mean)),
            'pos_feature_std': float(np.mean(pos_std)),
            'neg_feature_std': float(np.mean(neg_std))
        }
        
        return analysis_results

def main():
    """Main function"""
    print("ğŸ¨ EdgeSpark Feature Visualization Tool")
    print("=" * 50)
    
    # Create data loaders
    print("ğŸ“š Creating data loaders...")
    train_loader, val_loader, test_loader = create_dataloaders(
        "dataset/train_set.pkl",
        "dataset/valid_set.pkl", 
        "dataset/test_set.pkl",
        batch_size=32,
        max_points=1500,
        num_workers=2
    )
    
    # Find latest model
    model_path = None
    if os.path.exists("experiments"):
        exp_dirs = [d for d in os.listdir("experiments") if d.startswith("exp_")]
        if exp_dirs:
            latest_exp = sorted(exp_dirs)[-1]
            model_path = os.path.join("experiments", latest_exp, "best_model.pth")
            if not os.path.exists(model_path):
                model_path = None
    
    # Create visualizer
    visualizer = FeatureVisualizer(model_path=model_path)
    
    # Extract features
    print("ğŸ” Extracting features from validation set...")
    features, labels = visualizer.extract_features(val_loader, max_samples=300)
    
    # Analyze features
    analysis = visualizer.analyze_features(features, labels)
    
    print("\nğŸ“Š Feature Analysis Results:")
    print(f"  Samples: {analysis['num_samples']}")
    print(f"  Feature dimensions: {analysis['num_features']}")
    print(f"  Positive samples: {analysis['num_positive']}, Negative samples: {analysis['num_negative']}")
    print(f"  Inter-class distance: {analysis['inter_class_distance']:.4f}")
    print(f"  Intra-class distance: {analysis['avg_intra_class_distance']:.4f}")
    print(f"  Separability ratio: {analysis['separability_ratio']:.4f}")
    print(f"  Active features: {analysis['active_features']}")
    
    # Visualize features
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"feature_visualizations_{timestamp}"
    visualizer.visualize_features(features, labels, save_dir)
    
    # Save analysis results
    with open(os.path.join(save_dir, "feature_analysis.json"), 'w') as f:
        json.dump(analysis, f, indent=2)
    
    print(f"\nâœ… Feature visualization completed! Results saved to: {save_dir}")
    
    # Give suggestions if separability is low
    if analysis['separability_ratio'] < 1.0:
        print("\nâš ï¸  Low feature separability detected. Suggestions:")
        print("  1. Increase training epochs")
        print("  2. Adjust learning rate")
        print("  3. Modify network architecture")
        print("  4. Use contrastive learning")

if __name__ == "__main__":
    main()