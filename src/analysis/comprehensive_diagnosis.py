"""
EdgeSparkæ¨¡å‹è¡¨ç°è¯Šæ–­åˆ†æ
å…¨æ–¹ä½åˆ†ææ¨¡å‹æ€§èƒ½ä¸ä½³çš„åŸå› 
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
import os
import sys
from pathlib import Path
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from scipy.spatial.distance import pdist, squareform
from scipy.stats import ks_2samp
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'data'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'models'))

from improved_dataset_loader import ImprovedEdgeSparkDataset
from final_approach import EdgeMatchingNet

class ModelDiagnostics:
    """æ¨¡å‹è¯Šæ–­åˆ†æå™¨"""
    
    def __init__(self, data_paths, model_path=None):
        """
        åˆå§‹åŒ–è¯Šæ–­å™¨
        Args:
            data_paths: æ•°æ®æ–‡ä»¶è·¯å¾„å­—å…¸ {'train': ..., 'valid': ..., 'test': ...}
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        self.data_paths = data_paths
        self.model_path = model_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½æ•°æ®
        self.datasets = {}
        self.dataloaders = {}
        self._load_datasets()
        
        # åŠ è½½æ¨¡å‹
        self.model = None
        if model_path and os.path.exists(model_path):
            self._load_model()
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.analysis_results = {}
        
    def _load_datasets(self):
        """åŠ è½½æ•°æ®é›†"""
        print("ğŸ” åŠ è½½æ•°æ®é›†...")
        for split, path in self.data_paths.items():
            self.datasets[split] = ImprovedEdgeSparkDataset(
                path, max_points=1000, augment=False, sampling_strategy='ordered'
            )
            
            self.dataloaders[split] = torch.utils.data.DataLoader(
                self.datasets[split], batch_size=32, shuffle=False, num_workers=0
            )
            print(f"  {split}: {len(self.datasets[split])} æ ·æœ¬")
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {self.model_path}")
        self.model = EdgeMatchingNet(max_points=1000).to(self.device)
        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))
        self.model.eval()
    
    def analyze_data_quality(self):
        """1. æ•°æ®è´¨é‡åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®è´¨é‡åˆ†æ")
        print("="*60)
        
        results = {}
        
        for split, dataset in self.datasets.items():
            print(f"\nğŸ“‹ {split.upper()}æ•°æ®é›†åˆ†æ:")
            
            # åŸºæœ¬ç»Ÿè®¡
            total_samples = len(dataset)
            positive_samples = sum(1 for s in dataset.samples if s['label'] == 1)
            negative_samples = total_samples - positive_samples
            
            print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
            print(f"  æ­£æ ·æœ¬æ•°: {positive_samples} ({positive_samples/total_samples:.1%})")
            print(f"  è´Ÿæ ·æœ¬æ•°: {negative_samples} ({negative_samples/total_samples:.1%})")
            
            # è¾¹ç¼˜ç‚¹äº‘é•¿åº¦åˆ†æ
            point_lengths = []
            for i in range(min(1000, len(dataset))):  # é‡‡æ ·1000ä¸ªæ ·æœ¬åˆ†æ
                sample = dataset.samples[i]
                source_len = len(dataset.edge_points[sample['source_idx']])
                target_len = len(dataset.edge_points[sample['target_idx']])
                point_lengths.extend([source_len, target_len])
            
            print(f"  è¾¹ç¼˜ç‚¹äº‘é•¿åº¦ç»Ÿè®¡:")
            print(f"    å¹³å‡: {np.mean(point_lengths):.1f}")
            print(f"    ä¸­ä½æ•°: {np.median(point_lengths):.1f}")
            print(f"    æœ€å°å€¼: {np.min(point_lengths)}")
            print(f"    æœ€å¤§å€¼: {np.max(point_lengths)}")
            print(f"    æ ‡å‡†å·®: {np.std(point_lengths):.1f}")
            
            # æ£€æŸ¥æ•°æ®è´¨é‡é—®é¢˜
            empty_fragments = sum(1 for points in dataset.edge_points if len(points) == 0)
            tiny_fragments = sum(1 for points in dataset.edge_points if len(points) < 10)
            huge_fragments = sum(1 for points in dataset.edge_points if len(points) > 5000)
            
            print(f"  æ•°æ®è´¨é‡æ£€æŸ¥:")
            print(f"    ç©ºç¢ç‰‡: {empty_fragments}")
            print(f"    å¾®å°ç¢ç‰‡(<10ç‚¹): {tiny_fragments}")
            print(f"    å·¨å¤§ç¢ç‰‡(>5000ç‚¹): {huge_fragments}")
            
            results[split] = {
                'total_samples': total_samples,
                'positive_ratio': positive_samples / total_samples,
                'point_lengths': {
                    'mean': float(np.mean(point_lengths)),
                    'median': float(np.median(point_lengths)),
                    'std': float(np.std(point_lengths)),
                    'min': int(np.min(point_lengths)),
                    'max': int(np.max(point_lengths))
                },
                'quality_issues': {
                    'empty_fragments': empty_fragments,
                    'tiny_fragments': tiny_fragments,
                    'huge_fragments': huge_fragments
                }
            }
        
        self.analysis_results['data_quality'] = results
        return results
    
    def analyze_feature_distribution(self):
        """2. ç‰¹å¾åˆ†å¸ƒåˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ¯ ç‰¹å¾åˆ†å¸ƒåˆ†æ")
        print("="*60)
        
        # åˆ†æè¾¹ç¼˜ç‚¹äº‘çš„å‡ ä½•ç‰¹å¾
        results = {}
        
        for split, dataset in self.datasets.items():
            print(f"\nğŸ“ {split.upper()}å‡ ä½•ç‰¹å¾åˆ†æ:")
            
            # é‡‡æ ·éƒ¨åˆ†æ•°æ®è¿›è¡Œåˆ†æ
            sample_size = min(500, len(dataset))
            features = {
                'perimeters': [],
                'areas': [],
                'aspect_ratios': [],
                'curvatures': []
            }
            
            for i in range(sample_size):
                sample = dataset.samples[i]
                source_points = dataset.edge_points[sample['source_idx']]
                target_points = dataset.edge_points[sample['target_idx']]
                
                for points in [source_points, target_points]:
                    if len(points) > 3:
                        # è®¡ç®—å‘¨é•¿
                        perimeter = self._calculate_perimeter(points)
                        features['perimeters'].append(perimeter)
                        
                        # è®¡ç®—é¢ç§¯ï¼ˆå¦‚æœæ˜¯é—­åˆè½®å»“ï¼‰
                        area = self._calculate_area(points)
                        features['areas'].append(area)
                        
                        # è®¡ç®—å®½é«˜æ¯”
                        aspect_ratio = self._calculate_aspect_ratio(points)
                        features['aspect_ratios'].append(aspect_ratio)
                        
                        # è®¡ç®—å¹³å‡æ›²ç‡
                        curvature = self._calculate_mean_curvature(points)
                        features['curvatures'].append(curvature)
            
            for feature_name, values in features.items():
                if values:
                    print(f"  {feature_name}:")
                    print(f"    å¹³å‡å€¼: {np.mean(values):.3f}")
                    print(f"    æ ‡å‡†å·®: {np.std(values):.3f}")
                    print(f"    èŒƒå›´: [{np.min(values):.3f}, {np.max(values):.3f}]")
            
            results[split] = {k: {
                'mean': float(np.mean(v)) if v else 0,
                'std': float(np.std(v)) if v else 0,
                'min': float(np.min(v)) if v else 0,
                'max': float(np.max(v)) if v else 0
            } for k, v in features.items()}
        
        self.analysis_results['feature_distribution'] = results
        return results
    
    def _calculate_perimeter(self, points):
        """è®¡ç®—è½®å»“å‘¨é•¿"""
        if len(points) < 2:
            return 0
        diffs = np.diff(points, axis=0)
        distances = np.sqrt(np.sum(diffs**2, axis=1))
        return np.sum(distances)
    
    def _calculate_area(self, points):
        """è®¡ç®—è½®å»“é¢ç§¯ï¼ˆä½¿ç”¨é‹å¸¦å…¬å¼ï¼‰"""
        if len(points) < 3:
            return 0
        x, y = points[:, 0], points[:, 1]
        return 0.5 * abs(sum(x[i]*y[i+1] - x[i+1]*y[i] for i in range(-1, len(x)-1)))
    
    def _calculate_aspect_ratio(self, points):
        """è®¡ç®—è¾¹ç•Œæ¡†å®½é«˜æ¯”"""
        if len(points) < 2:
            return 1.0
        min_coords = np.min(points, axis=0)
        max_coords = np.max(points, axis=0)
        ranges = max_coords - min_coords
        return max(ranges) / (min(ranges) + 1e-8)
    
    def _calculate_mean_curvature(self, points):
        """è®¡ç®—å¹³å‡æ›²ç‡"""
        if len(points) < 3:
            return 0
        
        curvatures = []
        for i in range(len(points)):
            p1 = points[i-1]
            p2 = points[i]
            p3 = points[(i+1) % len(points)]
            
            # è®¡ç®—ä¸‰ç‚¹è§’åº¦å˜åŒ–
            v1 = p2 - p1
            v2 = p3 - p2
            
            # é¿å…é™¤é›¶
            len1 = np.linalg.norm(v1) + 1e-8
            len2 = np.linalg.norm(v2) + 1e-8
            
            cos_angle = np.dot(v1, v2) / (len1 * len2)
            cos_angle = np.clip(cos_angle, -1, 1)
            angle = np.arccos(cos_angle)
            curvatures.append(angle)
        
        return np.mean(curvatures)
    
    def analyze_model_behavior(self):
        """3. æ¨¡å‹è¡Œä¸ºåˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ¤– æ¨¡å‹è¡Œä¸ºåˆ†æ")
        print("="*60)
        
        if self.model is None:
            print("âŒ æ²¡æœ‰åŠ è½½æ¨¡å‹ï¼Œè·³è¿‡æ¨¡å‹è¡Œä¸ºåˆ†æ")
            return {}
        
        results = {}
        
        # å¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œé¢„æµ‹åˆ†æ
        for split, dataloader in self.dataloaders.items():
            print(f"\nğŸ” {split.upper()}æ•°æ®é›†æ¨¡å‹è¡Œä¸º:")
            
            predictions = []
            probabilities = []
            true_labels = []
            feature_embeddings = []
            
            self.model.eval()
            with torch.no_grad():
                for batch in dataloader:
                    source_points = batch['source_points'].to(self.device)
                    target_points = batch['target_points'].to(self.device)
                    labels = batch['label'].to(self.device)
                    
                    # è·å–ç‰¹å¾åµŒå…¥
                    source_features = self.model.shape_encoder(source_points)
                    target_features = self.model.shape_encoder(target_points)
                    
                    # è·å–é¢„æµ‹ç»“æœ
                    logits = self.model(source_points, target_points)
                    probs = torch.sigmoid(logits)
                    preds = (probs > 0.5).float()
                    
                    predictions.extend(preds.cpu().numpy())
                    probabilities.extend(probs.cpu().numpy())
                    true_labels.extend(labels.cpu().numpy())
                    
                    # ä¿å­˜ç‰¹å¾åµŒå…¥
                    combined_features = torch.cat([source_features, target_features], dim=1)
                    feature_embeddings.extend(combined_features.cpu().numpy())
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            predictions = np.array(predictions).flatten()
            probabilities = np.array(probabilities).flatten()
            true_labels = np.array(true_labels).flatten()
            feature_embeddings = np.array(feature_embeddings)
            
            # åˆ†æé¢„æµ‹åˆ†å¸ƒ
            print(f"  é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ:")
            print(f"    å¹³å‡é¢„æµ‹æ¦‚ç‡: {np.mean(probabilities):.3f}")
            print(f"    é¢„æµ‹æ¦‚ç‡æ ‡å‡†å·®: {np.std(probabilities):.3f}")
            print(f"    é¢„æµ‹æ¦‚ç‡èŒƒå›´: [{np.min(probabilities):.3f}, {np.max(probabilities):.3f}]")
            
            # åˆ†æé¢„æµ‹ç½®ä¿¡åº¦
            high_confidence = np.sum((probabilities < 0.2) | (probabilities > 0.8))
            low_confidence = np.sum((probabilities >= 0.4) & (probabilities <= 0.6))
            
            print(f"  é¢„æµ‹ç½®ä¿¡åº¦:")
            print(f"    é«˜ç½®ä¿¡åº¦é¢„æµ‹ (<0.2 or >0.8): {high_confidence} ({high_confidence/len(probabilities):.1%})")
            print(f"    ä½ç½®ä¿¡åº¦é¢„æµ‹ (0.4-0.6): {low_confidence} ({low_confidence/len(probabilities):.1%})")
            
            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(true_labels, predictions)
            print(f"  æ··æ·†çŸ©é˜µ:")
            print(f"    True Neg: {cm[0,0]}, False Pos: {cm[0,1]}")
            print(f"    False Neg: {cm[1,0]}, True Pos: {cm[1,1]}")
            
            # ç‰¹å¾åµŒå…¥åˆ†æ
            print(f"  ç‰¹å¾åµŒå…¥åˆ†æ:")
            print(f"    ç‰¹å¾ç»´åº¦: {feature_embeddings.shape[1]}")
            print(f"    ç‰¹å¾å‡å€¼: {np.mean(feature_embeddings):.3f}")
            print(f"    ç‰¹å¾æ ‡å‡†å·®: {np.std(feature_embeddings):.3f}")
            
            # åˆ†ææ­£è´Ÿæ ·æœ¬çš„ç‰¹å¾å·®å¼‚
            pos_indices = true_labels == 1
            neg_indices = true_labels == 0
            
            if np.sum(pos_indices) > 0 and np.sum(neg_indices) > 0:
                pos_features = feature_embeddings[pos_indices]
                neg_features = feature_embeddings[neg_indices]
                
                # è®¡ç®—æ­£è´Ÿæ ·æœ¬ç‰¹å¾çš„è·ç¦»
                pos_mean = np.mean(pos_features, axis=0)
                neg_mean = np.mean(neg_features, axis=0)
                feature_distance = np.linalg.norm(pos_mean - neg_mean)
                
                print(f"    æ­£è´Ÿæ ·æœ¬ç‰¹å¾è·ç¦»: {feature_distance:.3f}")
                
                # ç‰¹å¾åˆ†ç¦»åº¦åˆ†æï¼ˆä½¿ç”¨PCAï¼‰
                pca = PCA(n_components=2)
                features_2d = pca.fit_transform(feature_embeddings)
                pos_features_2d = features_2d[pos_indices]
                neg_features_2d = features_2d[neg_indices]
                
                # è®¡ç®—ç±»é—´/ç±»å†…è·ç¦»æ¯”
                if len(pos_features_2d) > 1 and len(neg_features_2d) > 1:
                    inter_class_dist = np.linalg.norm(np.mean(pos_features_2d, axis=0) - np.mean(neg_features_2d, axis=0))
                    intra_class_dist_pos = np.mean(pdist(pos_features_2d))
                    intra_class_dist_neg = np.mean(pdist(neg_features_2d))
                    avg_intra_class_dist = (intra_class_dist_pos + intra_class_dist_neg) / 2
                    
                    separability = inter_class_dist / (avg_intra_class_dist + 1e-8)
                    print(f"    ç‰¹å¾å¯åˆ†ç¦»æ€§ (ç±»é—´/ç±»å†…è·ç¦»æ¯”): {separability:.3f}")
            
            results[split] = {
                'prediction_stats': {
                    'mean_prob': float(np.mean(probabilities)),
                    'std_prob': float(np.std(probabilities)),
                    'high_confidence_ratio': float(high_confidence / len(probabilities)),
                    'low_confidence_ratio': float(low_confidence / len(probabilities))
                },
                'confusion_matrix': cm.tolist(),
                'feature_stats': {
                    'mean': float(np.mean(feature_embeddings)),
                    'std': float(np.std(feature_embeddings)),
                    'feature_distance': float(feature_distance) if 'feature_distance' in locals() else 0,
                    'separability': float(separability) if 'separability' in locals() else 0
                }
            }
        
        self.analysis_results['model_behavior'] = results
        return results
    
    def analyze_task_complexity(self):
        """4. ä»»åŠ¡å¤æ‚åº¦åˆ†æ"""
        print("\n" + "="*60)
        print("ğŸ¯ ä»»åŠ¡å¤æ‚åº¦åˆ†æ")
        print("="*60)
        
        results = {}
        
        # åˆ†æç¢ç‰‡é—´çš„ç›¸ä¼¼æ€§
        print("ğŸ” åˆ†æç¢ç‰‡é—´ç›¸ä¼¼æ€§...")
        
        # ä½¿ç”¨è®­ç»ƒé›†è¿›è¡Œåˆ†æ
        train_dataset = self.datasets['train']
        
        # é‡‡æ ·éƒ¨åˆ†æ•°æ®è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
        sample_size = min(100, len(train_dataset.edge_points))
        sampled_indices = np.random.choice(len(train_dataset.edge_points), sample_size, replace=False)
        
        # è®¡ç®—å‡ ä½•ç‰¹å¾
        geometric_features = []
        for idx in sampled_indices:
            points = train_dataset.edge_points[idx]
            if len(points) > 3:
                feature = [
                    self._calculate_perimeter(points),
                    self._calculate_area(points),
                    self._calculate_aspect_ratio(points),
                    self._calculate_mean_curvature(points),
                    len(points)
                ]
                geometric_features.append(feature)
            else:
                geometric_features.append([0, 0, 1, 0, len(points)])
        
        geometric_features = np.array(geometric_features)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        geometric_features_scaled = scaler.fit_transform(geometric_features)
        
        # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
        similarity_matrix = 1 / (1 + squareform(pdist(geometric_features_scaled, metric='euclidean')))
        
        # åˆ†æç›¸ä¼¼æ€§åˆ†å¸ƒ
        upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
        
        print(f"ç¢ç‰‡é—´ç›¸ä¼¼æ€§åˆ†æ:")
        print(f"  å¹³å‡ç›¸ä¼¼æ€§: {np.mean(upper_triangle):.3f}")
        print(f"  ç›¸ä¼¼æ€§æ ‡å‡†å·®: {np.std(upper_triangle):.3f}")
        print(f"  æœ€é«˜ç›¸ä¼¼æ€§: {np.max(upper_triangle):.3f}")
        print(f"  æœ€ä½ç›¸ä¼¼æ€§: {np.min(upper_triangle):.3f}")
        
        # åˆ†æé«˜ç›¸ä¼¼æ€§å¯¹çš„æ¯”ä¾‹
        high_similarity_threshold = 0.8
        high_similarity_pairs = np.sum(upper_triangle > high_similarity_threshold)
        total_pairs = len(upper_triangle)
        
        print(f"  é«˜ç›¸ä¼¼æ€§å¯¹(>{high_similarity_threshold}): {high_similarity_pairs}/{total_pairs} ({high_similarity_pairs/total_pairs:.1%})")
        
        # åˆ†ææ­£æ ·æœ¬å¯¹vsè´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼æ€§å·®å¼‚
        print("\nğŸ¯ æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼æ€§å¯¹æ¯”:")
        
        positive_similarities = []
        negative_similarities = []
        
        # è®¡ç®—æ­£æ ·æœ¬å¯¹çš„ç›¸ä¼¼æ€§
        for source_idx, target_idx in train_dataset.gt_pairs[:50]:  # é‡‡æ ·50å¯¹
            if source_idx < len(geometric_features) and target_idx < len(geometric_features):
                sim = 1 / (1 + np.linalg.norm(geometric_features_scaled[source_idx] - geometric_features_scaled[target_idx]))
                positive_similarities.append(sim)
        
        # è®¡ç®—è´Ÿæ ·æœ¬å¯¹çš„ç›¸ä¼¼æ€§ï¼ˆéšæœºé‡‡æ ·ï¼‰
        for _ in range(min(50, len(positive_similarities))):
            idx1, idx2 = np.random.choice(len(geometric_features), 2, replace=False)
            sim = 1 / (1 + np.linalg.norm(geometric_features_scaled[idx1] - geometric_features_scaled[idx2]))
            negative_similarities.append(sim)
        
        if positive_similarities and negative_similarities:
            print(f"  æ­£æ ·æœ¬å¯¹å¹³å‡ç›¸ä¼¼æ€§: {np.mean(positive_similarities):.3f}")
            print(f"  è´Ÿæ ·æœ¬å¯¹å¹³å‡ç›¸ä¼¼æ€§: {np.mean(negative_similarities):.3f}")
            
            # è¿›è¡Œç»Ÿè®¡æ£€éªŒ
            ks_stat, p_value = ks_2samp(positive_similarities, negative_similarities)
            print(f"  KSç»Ÿè®¡é‡: {ks_stat:.3f}, på€¼: {p_value:.3f}")
            
            if p_value > 0.05:
                print("  âš ï¸  æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼æ€§åˆ†å¸ƒæ— æ˜¾è‘—å·®å¼‚ï¼Œä»»åŠ¡å¯èƒ½å¾ˆå›°éš¾")
            else:
                print("  âœ… æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼æ€§åˆ†å¸ƒæœ‰æ˜¾è‘—å·®å¼‚")
        
        # åˆ†ææ•°æ®çš„å†…åœ¨ç»´åº¦
        print(f"\nğŸ“ æ•°æ®å†…åœ¨ç»´åº¦åˆ†æ:")
        pca = PCA()
        pca.fit(geometric_features_scaled)
        
        # è®¡ç®—è§£é‡Šæ–¹å·®æ¯”
        cumsum_ratio = np.cumsum(pca.explained_variance_ratio_)
        intrinsic_dim = np.argmax(cumsum_ratio >= 0.95) + 1
        
        print(f"  95%æ–¹å·®è§£é‡Šæ‰€éœ€ç»´åº¦: {intrinsic_dim}")
        print(f"  å‰3ä¸ªä¸»æˆåˆ†è§£é‡Šæ–¹å·®: {cumsum_ratio[2]:.1%}")
        
        results = {
            'similarity_stats': {
                'mean_similarity': float(np.mean(upper_triangle)),
                'std_similarity': float(np.std(upper_triangle)),
                'high_similarity_ratio': float(high_similarity_pairs / total_pairs)
            },
            'class_similarity': {
                'positive_mean': float(np.mean(positive_similarities)) if positive_similarities else 0,
                'negative_mean': float(np.mean(negative_similarities)) if negative_similarities else 0,
                'ks_pvalue': float(p_value) if 'p_value' in locals() else 1.0
            },
            'intrinsic_dimension': int(intrinsic_dim),
            'pca_variance_ratio': cumsum_ratio[2] if len(cumsum_ratio) > 2 else 0
        }
        
        self.analysis_results['task_complexity'] = results
        return results
    
    def implement_baseline_methods(self):
        """5. åŸºçº¿æ–¹æ³•å¯¹æ¯”"""
        print("\n" + "="*60)
        print("ğŸ“ åŸºçº¿æ–¹æ³•å¯¹æ¯”")
        print("="*60)
        
        results = {}
        
        # 1. éšæœºåŸºçº¿
        print("ğŸ² éšæœºåŸºçº¿:")
        random_accuracy = 0.5  # å¹³è¡¡æ•°æ®é›†çš„éšæœºå‡†ç¡®ç‡
        print(f"  éšæœºå‡†ç¡®ç‡: {random_accuracy:.3f}")
        
        # 2. ç®€å•å‡ ä½•ç‰¹å¾åŸºçº¿
        print("\nğŸ“ å‡ ä½•ç‰¹å¾åŸºçº¿:")
        
        # ä½¿ç”¨éªŒè¯é›†æµ‹è¯•åŸºçº¿æ–¹æ³•
        val_dataset = self.datasets['valid']
        
        correct_predictions = 0
        total_predictions = 0
        
        for sample in val_dataset.samples[:100]:  # æµ‹è¯•100ä¸ªæ ·æœ¬
            source_points = val_dataset.edge_points[sample['source_idx']]
            target_points = val_dataset.edge_points[sample['target_idx']]
            true_label = sample['label']
            
            # è®¡ç®—å‡ ä½•ç‰¹å¾ç›¸ä¼¼æ€§
            if len(source_points) > 3 and len(target_points) > 3:
                source_features = np.array([
                    self._calculate_perimeter(source_points),
                    self._calculate_area(target_points),
                    self._calculate_aspect_ratio(source_points),
                    len(source_points)
                ])
                
                target_features = np.array([
                    self._calculate_perimeter(target_points),
                    self._calculate_area(target_points),
                    self._calculate_aspect_ratio(target_points),
                    len(target_points)
                ])
                
                # è®¡ç®—ç›¸ä¼¼æ€§ï¼ˆå½’ä¸€åŒ–åçš„L2è·ç¦»ï¼‰
                source_features = source_features / (np.linalg.norm(source_features) + 1e-8)
                target_features = target_features / (np.linalg.norm(target_features) + 1e-8)
                
                similarity = 1 / (1 + np.linalg.norm(source_features - target_features))
                prediction = 1 if similarity > 0.7 else 0
                
                if prediction == true_label:
                    correct_predictions += 1
                total_predictions += 1
        
        geometric_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        print(f"  å‡ ä½•ç‰¹å¾åŸºçº¿å‡†ç¡®ç‡: {geometric_accuracy:.3f}")
        
        # 3. é•¿åº¦æ¯”åŸºçº¿
        print("\nğŸ“ é•¿åº¦æ¯”åŸºçº¿:")
        
        correct_predictions = 0
        total_predictions = 0
        
        for sample in val_dataset.samples[:100]:
            source_len = len(val_dataset.edge_points[sample['source_idx']])
            target_len = len(val_dataset.edge_points[sample['target_idx']])
            true_label = sample['label']
            
            # é•¿åº¦ç›¸ä¼¼æ€§
            length_ratio = min(source_len, target_len) / (max(source_len, target_len) + 1e-8)
            prediction = 1 if length_ratio > 0.8 else 0
            
            if prediction == true_label:
                correct_predictions += 1
            total_predictions += 1
        
        length_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
        print(f"  é•¿åº¦æ¯”åŸºçº¿å‡†ç¡®ç‡: {length_accuracy:.3f}")
        
        # å¯¹æ¯”æ·±åº¦å­¦ä¹ æ¨¡å‹
        if self.model is not None and 'valid' in self.analysis_results.get('model_behavior', {}):
            model_accuracy = 1 - np.mean(self.analysis_results['model_behavior']['valid']['confusion_matrix'])
            model_accuracy = (self.analysis_results['model_behavior']['valid']['confusion_matrix'][0][0] + 
                            self.analysis_results['model_behavior']['valid']['confusion_matrix'][1][1]) / \
                           np.sum(self.analysis_results['model_behavior']['valid']['confusion_matrix'])
            
            print(f"\nğŸ¤– æ·±åº¦å­¦ä¹ æ¨¡å‹å‡†ç¡®ç‡: {model_accuracy:.3f}")
            
            print("\nğŸ“Š åŸºçº¿å¯¹æ¯”æ€»ç»“:")
            baselines = [
                ("éšæœºåŸºçº¿", random_accuracy),
                ("å‡ ä½•ç‰¹å¾åŸºçº¿", geometric_accuracy),
                ("é•¿åº¦æ¯”åŸºçº¿", length_accuracy),
                ("æ·±åº¦å­¦ä¹ æ¨¡å‹", model_accuracy)
            ]
            
            for name, acc in baselines:
                print(f"  {name}: {acc:.3f}")
        
        results = {
            'random_baseline': random_accuracy,
            'geometric_baseline': geometric_accuracy,
            'length_baseline': length_accuracy,
            'model_accuracy': model_accuracy if 'model_accuracy' in locals() else 0
        }
        
        self.analysis_results['baseline_comparison'] = results
        return results
    
    def analyze_failure_cases(self):
        """6. é”™è¯¯æ¡ˆä¾‹åˆ†æ"""
        print("\n" + "="*60)
        print("âŒ é”™è¯¯æ¡ˆä¾‹åˆ†æ")
        print("="*60)
        
        if self.model is None:
            print("âŒ æ²¡æœ‰åŠ è½½æ¨¡å‹ï¼Œè·³è¿‡é”™è¯¯æ¡ˆä¾‹åˆ†æ")
            return {}
        
        results = {}
        
        # åœ¨éªŒè¯é›†ä¸Šæ‰¾é”™è¯¯æ¡ˆä¾‹
        val_dataloader = self.dataloaders['valid']
        
        false_positives = []  # å‡é˜³æ€§ï¼šæ¨¡å‹é¢„æµ‹ä¸ºåŒ¹é…ï¼Œå®é™…ä¸åŒ¹é…
        false_negatives = []  # å‡é˜´æ€§ï¼šæ¨¡å‹é¢„æµ‹ä¸ºä¸åŒ¹é…ï¼Œå®é™…åŒ¹é…
        
        self.model.eval()
        with torch.no_grad():
            for batch_idx, batch in enumerate(val_dataloader):
                source_points = batch['source_points'].to(self.device)
                target_points = batch['target_points'].to(self.device)
                labels = batch['label'].to(self.device)
                source_indices = batch['source_idx'].numpy()
                target_indices = batch['target_idx'].numpy()
                
                logits = self.model(source_points, target_points)
                probs = torch.sigmoid(logits)
                preds = (probs > 0.5).float()
                
                # æ‰¾é”™è¯¯æ¡ˆä¾‹
                for i in range(len(labels)):
                    true_label = labels[i].item()
                    pred_label = preds[i].item()
                    confidence = probs[i].item()
                    
                    if true_label != pred_label:
                        case_info = {
                            'source_idx': int(source_indices[i]),
                            'target_idx': int(target_indices[i]),
                            'true_label': int(true_label),
                            'pred_label': int(pred_label),
                            'confidence': float(confidence),
                            'source_points': source_points[i].cpu().numpy(),
                            'target_points': target_points[i].cpu().numpy()
                        }
                        
                        if true_label == 0 and pred_label == 1:
                            false_positives.append(case_info)
                        elif true_label == 1 and pred_label == 0:
                            false_negatives.append(case_info)
                
                # é™åˆ¶åˆ†æçš„æ¡ˆä¾‹æ•°é‡
                if len(false_positives) + len(false_negatives) >= 50:
                    break
        
        print(f"æ”¶é›†åˆ°é”™è¯¯æ¡ˆä¾‹: FP={len(false_positives)}, FN={len(false_negatives)}")
        
        # åˆå§‹åŒ–æ‰€æœ‰å˜é‡
        fp_length_ratios = []
        fp_area_ratios = []
        fn_length_ratios = []
        fn_area_ratios = []
        
        # åˆ†æå‡é˜³æ€§æ¡ˆä¾‹
        if false_positives:
            print(f"\nğŸ”´ å‡é˜³æ€§åˆ†æ (é¢„æµ‹åŒ¹é…ï¼Œå®é™…ä¸åŒ¹é…):")
            fp_confidences = [case['confidence'] for case in false_positives]
            print(f"  æ•°é‡: {len(false_positives)}")
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(fp_confidences):.3f}")
            print(f"  ç½®ä¿¡åº¦èŒƒå›´: [{np.min(fp_confidences):.3f}, {np.max(fp_confidences):.3f}]")
            
            # åˆ†æå‡é˜³æ€§æ¡ˆä¾‹çš„å‡ ä½•ç‰¹å¾
            fp_length_ratios = []
            fp_area_ratios = []
            
            for case in false_positives[:10]:  # åˆ†æå‰10ä¸ªæ¡ˆä¾‹
                val_dataset = self.datasets['valid']
                source_points = val_dataset.edge_points[case['source_idx']]
                target_points = val_dataset.edge_points[case['target_idx']]
                
                if len(source_points) > 3 and len(target_points) > 3:
                    source_len = len(source_points)
                    target_len = len(target_points)
                    length_ratio = min(source_len, target_len) / max(source_len, target_len)
                    fp_length_ratios.append(length_ratio)
                    
                    source_area = self._calculate_area(source_points)
                    target_area = self._calculate_area(target_points)
                    if source_area > 0 and target_area > 0:
                        area_ratio = min(source_area, target_area) / max(source_area, target_area)
                        fp_area_ratios.append(area_ratio)
            
            if fp_length_ratios:
                print(f"  é•¿åº¦ç›¸ä¼¼æ€§: {np.mean(fp_length_ratios):.3f} Â± {np.std(fp_length_ratios):.3f}")
            if fp_area_ratios:
                print(f"  é¢ç§¯ç›¸ä¼¼æ€§: {np.mean(fp_area_ratios):.3f} Â± {np.std(fp_area_ratios):.3f}")
        
        # åˆ†æå‡é˜´æ€§æ¡ˆä¾‹
        if false_negatives:
            print(f"\nğŸ”µ å‡é˜´æ€§åˆ†æ (é¢„æµ‹ä¸åŒ¹é…ï¼Œå®é™…åŒ¹é…):")
            fn_confidences = [1 - case['confidence'] for case in false_negatives]  # è½¬æ¢ä¸º"ä¸åŒ¹é…"çš„ç½®ä¿¡åº¦
            print(f"  æ•°é‡: {len(false_negatives)}")
            print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(fn_confidences):.3f}")
            print(f"  ç½®ä¿¡åº¦èŒƒå›´: [{np.min(fn_confidences):.3f}, {np.max(fn_confidences):.3f}]")
            
            # åˆ†æå‡é˜´æ€§æ¡ˆä¾‹çš„å‡ ä½•ç‰¹å¾
            fn_length_ratios = []
            fn_area_ratios = []
            
            for case in false_negatives[:10]:
                val_dataset = self.datasets['valid']
                source_points = val_dataset.edge_points[case['source_idx']]
                target_points = val_dataset.edge_points[case['target_idx']]
                
                if len(source_points) > 3 and len(target_points) > 3:
                    source_len = len(source_points)
                    target_len = len(target_points)
                    length_ratio = min(source_len, target_len) / max(source_len, target_len)
                    fn_length_ratios.append(length_ratio)
                    
                    source_area = self._calculate_area(source_points)
                    target_area = self._calculate_area(target_points)
                    if source_area > 0 and target_area > 0:
                        area_ratio = min(source_area, target_area) / max(source_area, target_area)
                        fn_area_ratios.append(area_ratio)
            
            if fn_length_ratios:
                print(f"  é•¿åº¦ç›¸ä¼¼æ€§: {np.mean(fn_length_ratios):.3f} Â± {np.std(fn_length_ratios):.3f}")
            if fn_area_ratios:
                print(f"  é¢ç§¯ç›¸ä¼¼æ€§: {np.mean(fn_area_ratios):.3f} Â± {np.std(fn_area_ratios):.3f}")
        
        results = {
            'false_positives': {
                'count': len(false_positives),
                'avg_confidence': float(np.mean(fp_confidences)) if false_positives else 0,
                'avg_length_similarity': float(np.mean(fp_length_ratios)) if fp_length_ratios else 0,
                'avg_area_similarity': float(np.mean(fp_area_ratios)) if fp_area_ratios else 0
            },
            'false_negatives': {
                'count': len(false_negatives),
                'avg_confidence': float(np.mean(fn_confidences)) if false_negatives else 0,
                'avg_length_similarity': float(np.mean(fn_length_ratios)) if fn_length_ratios else 0,
                'avg_area_similarity': float(np.mean(fn_area_ratios)) if fn_area_ratios else 0
            }
        }
        
        self.analysis_results['failure_analysis'] = results
        return results
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆè¯Šæ–­æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“‹ EdgeSparkæ¨¡å‹è¯Šæ–­ç»¼åˆæŠ¥å‘Š")
        print("="*80)
        
        # è¿è¡Œæ‰€æœ‰åˆ†æ
        print("ğŸ” å¼€å§‹å…¨é¢è¯Šæ–­åˆ†æ...")
        
        self.analyze_data_quality()
        self.analyze_feature_distribution()
        self.analyze_model_behavior()
        self.analyze_task_complexity()
        self.implement_baseline_methods()
        self.analyze_failure_cases()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        print("\n" + "="*80)
        print("ğŸ“Š è¯Šæ–­æ€»ç»“ä¸å»ºè®®")
        print("="*80)
        
        # æ•°æ®è´¨é‡é—®é¢˜
        print("\n1ï¸âƒ£ æ•°æ®è´¨é‡è¯Šæ–­:")
        data_quality = self.analysis_results.get('data_quality', {})
        if data_quality:
            train_quality = data_quality.get('train', {})
            if train_quality.get('quality_issues', {}).get('tiny_fragments', 0) > 100:
                print("  âš ï¸  å‘ç°å¤§é‡å¾®å°ç¢ç‰‡ï¼Œå¯èƒ½å½±å“ç‰¹å¾å­¦ä¹ ")
            if train_quality.get('point_lengths', {}).get('std', 0) > 1000:
                print("  âš ï¸  ç¢ç‰‡å°ºå¯¸å˜åŒ–å¾ˆå¤§ï¼Œå»ºè®®æ”¹è¿›å½’ä¸€åŒ–ç­–ç•¥")
            print("  âœ… æ­£è´Ÿæ ·æœ¬å¹³è¡¡è‰¯å¥½")
        
        # ç‰¹å¾åˆ†ç¦»åº¦é—®é¢˜
        print("\n2ï¸âƒ£ ç‰¹å¾è¡¨ç¤ºè¯Šæ–­:")
        model_behavior = self.analysis_results.get('model_behavior', {})
        if model_behavior:
            val_behavior = model_behavior.get('valid', {})
            separability = val_behavior.get('feature_stats', {}).get('separability', 0)
            if separability < 1.0:
                print("  âŒ ç‰¹å¾å¯åˆ†ç¦»æ€§å·®ï¼Œæ­£è´Ÿæ ·æœ¬åœ¨ç‰¹å¾ç©ºé—´ä¸­éš¾ä»¥åŒºåˆ†")
                print("     å»ºè®®ï¼šå¢å¼ºç‰¹å¾æå–èƒ½åŠ›ï¼Œå°è¯•æ›´å¤æ‚çš„ç½‘ç»œæ¶æ„")
            else:
                print("  âœ… ç‰¹å¾å¯åˆ†ç¦»æ€§è‰¯å¥½")
        
        # ä»»åŠ¡å¤æ‚åº¦é—®é¢˜
        print("\n3ï¸âƒ£ ä»»åŠ¡å¤æ‚åº¦è¯Šæ–­:")
        task_complexity = self.analysis_results.get('task_complexity', {})
        if task_complexity:
            ks_pvalue = task_complexity.get('class_similarity', {}).get('ks_pvalue', 1.0)
            if ks_pvalue > 0.05:
                print("  âŒ æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼æ€§åˆ†å¸ƒæ— æ˜¾è‘—å·®å¼‚ï¼Œä»»åŠ¡æœ¬èº«å¾ˆå›°éš¾")
                print("     å»ºè®®ï¼šæ”¶é›†æ›´å¤šé«˜è´¨é‡çš„åŒ¹é…æ ·æœ¬ï¼Œæˆ–é‡æ–°å®šä¹‰åŒ¹é…æ ‡å‡†")
            else:
                print("  âœ… ä»»åŠ¡å…·æœ‰å¯å­¦ä¹ æ€§")
        
        # åŸºçº¿å¯¹æ¯”
        print("\n4ï¸âƒ£ æ¨¡å‹æ€§èƒ½è¯Šæ–­:")
        baseline_comparison = self.analysis_results.get('baseline_comparison', {})
        if baseline_comparison:
            model_acc = baseline_comparison.get('model_accuracy', 0)
            geometric_acc = baseline_comparison.get('geometric_baseline', 0)
            
            if model_acc < geometric_acc + 0.1:
                print("  âŒ æ·±åº¦å­¦ä¹ æ¨¡å‹æœªèƒ½æ˜¾è‘—è¶…è¶Šç®€å•åŸºçº¿")
                print("     å»ºè®®ï¼šæ£€æŸ¥ç½‘ç»œæ¶æ„è®¾è®¡ï¼Œå¢åŠ æ¨¡å‹å¤æ‚åº¦")
            else:
                print("  âœ… æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½ä¼˜äºåŸºçº¿")
        
        # é”™è¯¯åˆ†æ
        print("\n5ï¸âƒ£ é”™è¯¯æ¨¡å¼è¯Šæ–­:")
        failure_analysis = self.analysis_results.get('failure_analysis', {})
        if failure_analysis:
            fp_count = failure_analysis.get('false_positives', {}).get('count', 0)
            fn_count = failure_analysis.get('false_negatives', {}).get('count', 0)
            
            if fp_count > fn_count:
                print("  âš ï¸  å‡é˜³æ€§é”™è¯¯è¾ƒå¤šï¼Œæ¨¡å‹è¿‡äºå®½æ¾")
                print("     å»ºè®®ï¼šè°ƒæ•´å†³ç­–é˜ˆå€¼ï¼Œå¢åŠ è´Ÿæ ·æœ¬è®­ç»ƒ")
            elif fn_count > fp_count:
                print("  âš ï¸  å‡é˜´æ€§é”™è¯¯è¾ƒå¤šï¼Œæ¨¡å‹è¿‡äºä¸¥æ ¼")
                print("     å»ºè®®ï¼šå¢å¼ºæ•°æ®å¢å¼ºï¼Œæé«˜æ¨¡å‹å¯¹å˜åŒ–çš„é²æ£’æ€§")
        
        # æ€»ä½“å»ºè®®
        print("\nğŸ”§ æ”¹è¿›å»ºè®®ä¼˜å…ˆçº§:")
        print("  1. æ•°æ®è´¨é‡ï¼šæ¸…ç†å¾®å°ç¢ç‰‡ï¼Œç»Ÿä¸€ç¢ç‰‡å°ºå¯¸èŒƒå›´")
        print("  2. ç‰¹å¾å·¥ç¨‹ï¼šå°è¯•æ›´ä¸°å¯Œçš„å‡ ä½•ç‰¹å¾ï¼ˆæ›²ç‡ã€å‚…é‡Œå¶æè¿°å­ç­‰ï¼‰")
        print("  3. ç½‘ç»œæ¶æ„ï¼šè€ƒè™‘ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æˆ–å›¾ç¥ç»ç½‘ç»œ")
        print("  4. è®­ç»ƒç­–ç•¥ï¼šå®æ–½è¯¾ç¨‹å­¦ä¹ ï¼Œä»ç®€å•æ ·æœ¬å¼€å§‹è®­ç»ƒ")
        print("  5. æ•°æ®å¢å¼ºï¼šå¢åŠ æ›´å¤šå‡ ä½•å˜æ¢ï¼ˆç¼©æ”¾ã€æ—‹è½¬ã€å™ªå£°ï¼‰")
        print("  6. æŸå¤±å‡½æ•°ï¼šå°è¯•focal losså¤„ç†å›°éš¾æ ·æœ¬")
        
        # ä¿å­˜æŠ¥å‘Š
        import json
        from datetime import datetime
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"diagnosis_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“ è¯¦ç»†è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return self.analysis_results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”¬ EdgeSparkæ¨¡å‹è¯Šæ–­åˆ†æ")
    print("="*50)
    
    # æ•°æ®è·¯å¾„
    data_paths = {
        'train': 'dataset/train_set.pkl',
        'valid': 'dataset/valid_set.pkl',
        'test': 'dataset/test_set.pkl'
    }
    
    # æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    model_path = 'best_final_model_ordered.pth'
    if not os.path.exists(model_path):
        model_path = None
        print("âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå°†è·³è¿‡æ¨¡å‹ç›¸å…³åˆ†æ")
    
    # åˆ›å»ºè¯Šæ–­å™¨
    diagnostics = ModelDiagnostics(data_paths, model_path)
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    results = diagnostics.generate_comprehensive_report()
    
    return results

if __name__ == "__main__":
    results = main()