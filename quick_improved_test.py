#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•æ”¹è¿›æ–¹æ¡ˆ
"""
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score
import numpy as np
import time

from hybrid_approach.improved_approach import ImprovedEdgeMatchingNet
from simplified_approach.dataset_simple import create_simple_dataloaders

def quick_test():
    """å¿«é€Ÿæµ‹è¯•æ”¹è¿›æ–¹æ¡ˆ"""
    print("ğŸš€ å¿«é€Ÿæµ‹è¯•æ”¹è¿›æ–¹æ¡ˆ")
    print("=" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ“š åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader, test_loader = create_simple_dataloaders(
        "dataset/train_set.pkl",
        "dataset/valid_set.pkl",
        "dataset/test_set.pkl",
        batch_size=32,
        max_points=1000,
        num_workers=4
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ§  åˆ›å»ºæ”¹è¿›æ¨¡å‹...")
    model = ImprovedEdgeMatchingNet(max_points=1000, feature_dim=128).to(device)
    
    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
    criterion = nn.BCEWithLogitsLoss()
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.5)
    
    print(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒ
    epochs = 15
    best_acc = 0.0
    best_f1 = 0.0
    best_auc = 0.0
    
    for epoch in range(epochs):
        epoch_start = time.time()
        
        # è®­ç»ƒ
        model.train()
        train_loss = 0.0
        train_preds = []
        train_labels = []
        
        for batch_idx, batch in enumerate(train_loader):
            points1 = batch['source_points'].to(device)
            points2 = batch['target_points'].to(device)
            labels = batch['label'].to(device)
            
            optimizer.zero_grad()
            logits = model(points1, points2)
            loss = criterion(logits, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item()
            probs = torch.sigmoid(logits)
            preds = (probs > 0.5).float()
            train_preds.extend(preds.cpu().numpy())
            train_labels.extend(labels.cpu().numpy())
            
            if batch_idx % 50 == 0:
                print(f'     Batch {batch_idx:3d}/{len(train_loader):3d} | Loss: {loss.item():.4f}')
        
        scheduler.step()
        
        train_acc = accuracy_score(train_labels, train_preds)
        train_loss /= len(train_loader)
        
        # éªŒè¯
        model.eval()
        val_loss = 0.0
        val_preds = []
        val_probs = []
        val_labels = []
        
        with torch.no_grad():
            for batch in val_loader:
                points1 = batch['source_points'].to(device)
                points2 = batch['target_points'].to(device)
                labels = batch['label'].to(device)
                
                logits = model(points1, points2)
                loss = criterion(logits, labels)
                val_loss += loss.item()
                
                probs = torch.sigmoid(logits)
                preds = (probs > 0.5).float()
                val_preds.extend(preds.cpu().numpy())
                val_probs.extend(probs.cpu().numpy())
                val_labels.extend(labels.cpu().numpy())
        
        val_loss /= len(val_loader)
        val_acc = accuracy_score(val_labels, val_preds)
        
        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
        try:
            precision, recall, f1, _ = precision_recall_fscore_support(
                val_labels, val_preds, average='binary', zero_division=0
            )
            auc = roc_auc_score(val_labels, val_probs)
        except:
            precision, recall, f1, auc = 0.0, 0.0, 0.0, 0.5
        
        # æ›´æ–°æœ€ä½³ç»“æœ
        if val_acc > best_acc:
            best_acc = val_acc
            best_f1 = f1
            best_auc = auc
        
        epoch_time = time.time() - epoch_start
        
        print(f'   Epoch {epoch+1:2d}: Train={train_acc:.4f}, Val={val_acc:.4f}, '
              f'F1={f1:.4f}, AUC={auc:.4f}, Time={epoch_time:.1f}s')
    
    print(f"\nğŸ† æœ€ä½³ç»“æœ:")
    print(f"   å‡†ç¡®ç‡: {best_acc:.4f}")
    print(f"   F1-Score: {best_f1:.4f}")
    print(f"   AUC: {best_auc:.4f}")
    
    # ä¸å·²çŸ¥ç»“æœå¯¹æ¯”
    print(f"\nğŸ“Š ä¸å·²çŸ¥ç»“æœå¯¹æ¯”:")
    comparisons = [
        ('åŸå§‹å¤æ‚ç½‘ç»œ', 0.5000),
        ('ç®€åŒ–ç½‘ç»œ', 0.5985),
        ('final_approach', 0.6095),
        ('æ··åˆæ–¹æ³•(å•é‡‡æ ·)', 0.5839),
        ('æ”¹è¿›æ–¹æ¡ˆ', best_acc)
    ]
    
    print("æ–¹æ³•                    | å‡†ç¡®ç‡")
    print("-" * 35)
    for method, acc in comparisons:
        if method == 'æ”¹è¿›æ–¹æ¡ˆ':
            print(f"{method:<20} | {acc:.4f} ğŸ†")
        else:
            print(f"{method:<20} | {acc:.4f}")
    
    # åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ”¹è¿›
    baseline_acc = 0.6095  # final_approach
    improvement = best_acc - baseline_acc
    
    print(f"\nğŸ“ˆ æ”¹è¿›åˆ†æ:")
    print(f"   ä¸final_approachå¯¹æ¯”: {improvement:+.4f} ({improvement/baseline_acc*100:+.1f}%)")
    
    if improvement > 0.01:
        print("   âœ… æ˜¾è‘—æ”¹è¿›ï¼Œæ–¹æ¡ˆæœ‰æ•ˆ")
        return True, best_acc
    elif improvement > 0.005:
        print("   âš ï¸ å°å¹…æ”¹è¿›ï¼Œå¯è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–")
        return False, best_acc
    else:
        print("   âŒ æ— æ˜æ˜¾æ”¹è¿›ï¼Œéœ€è¦é‡æ–°è®¾è®¡")
        return False, best_acc

if __name__ == "__main__":
    success, acc = quick_test()